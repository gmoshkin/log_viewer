[GovernorTest] === TEST STARTED ===
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] === TEST STARTED ===
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
[GovernorTest] Random Seed: K1dqiBp0 (use this for reproduction)
[GovernorTest][Phase 1/5] Initial cluster setup
[GovernorTest] Creating cluster:
[GovernorTest]   - Node count: 3
[GovernorTest]   - Replication factor: 3
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'cluster.name': "cluster-0-0"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'cluster.default_replication_factor': 3
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.instance_dir': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.service_password_file': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/password.txt"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.cluster_name': "cluster-0-0"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.tier': "default"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.failure_domain': {}
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.iproto_listen': "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.iproto_advertise': "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.peer': ["127.0.0.1:3300"]
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.admin_socket': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/admin.sock"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.share_dir': "/home/gmoshkin/code/pd/picodata/test/testplug"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.audit': "/dev/stderr"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.shredding': false
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.log.level': "verbose"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.log.format': "plain"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.memtx.memory': "64M"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.memory': "128M"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.cache': "128M"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.bloom_fpr': 0.05
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.max_tuple_size': "1M"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.page_size': "8K"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.range_size': "1G"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.run_count_per_level': 2
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.run_size_ratio': 3.5
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.read_threads': 1
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.write_threads': 4
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.vinyl.timeout': 60
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.pg.listen': "127.0.0.1:3301"
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> 'instance.pg.ssl': false
:3300 | 2025-02-17 23:38:27.164 [771631] main/104/interactive I> entering discovery phase
:3300 | 2025-02-17 23:38:27.165 [771631] main/104/interactive I> error injection enabled
:3300 | 2025-02-17 23:38:27.165 [771631] main/104/interactive I> Tarantool 2.11.5-165-gfae9c5bbe Linux-x86_64-RelWithDebInfo
:3300 | 2025-02-17 23:38:27.165 [771631] main/104/interactive I> log level 6
:3300 | 2025-02-17 23:38:27.165 [771631] main/104/interactive I> wal/engine cleanup is paused
:3300 | 2025-02-17 23:38:27.166 [771631] main/104/interactive I> mapping 67108864 bytes for memtx tuple arena...
:3300 | 2025-02-17 23:38:27.166 [771631] main/104/interactive I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
:3300 | 2025-02-17 23:38:27.166 [771631] main/104/interactive I> mapping 134217728 bytes for vinyl tuple arena...
:3300 | 2025-02-17 23:38:27.167 [771631] main/104/interactive I> update replication_synchro_quorum = 1
:3300 | 2025-02-17 23:38:27.167 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.167 [771631] main/104/interactive I> instance uuid ce7caf19-d3dc-4179-99e1-7c246d997593
:3300 | 2025-02-17 23:38:27.167 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.167 [771631] main/104/interactive I> initializing an empty data directory
:3300 | 2025-02-17 23:38:27.178 [771631] main/104/interactive V> RAFT: recover {term: 1}
:3300 | 2025-02-17 23:38:27.178 [771631] main/104/interactive I> assigned id 1 to replica ce7caf19-d3dc-4179-99e1-7c246d997593
:3300 | 2025-02-17 23:38:27.178 [771631] main/104/interactive I> update replication_synchro_quorum = 1
:3300 | 2025-02-17 23:38:27.178 [771631] main/104/interactive I> cluster uuid 1612a1f3-5a1c-4147-9b21-662c3af85b5f
:3300 | 2025-02-17 23:38:27.178 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.178 [771631] snapshot/101/main I> saving snapshot `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.snap.inprogress'
:3300 | 2025-02-17 23:38:27.181 [771631] snapshot/101/main I> done
:3300 | 2025-02-17 23:38:27.182 [771631] main/104/interactive I> ready to accept requests
:3300 | 2025-02-17 23:38:27.182 [771631] main/105/gc I> wal/engine cleanup is resumed
:3300 | 2025-02-17 23:38:27.182 [771631] main/104/interactive/box.load_cfg I> set 'memtx_memory' configuration option to 67108864
:3300 | 2025-02-17 23:38:27.182 [771631] main/104/interactive/box.load_cfg I> set 'log_level' configuration option to 6
:3300 | 2025-02-17 23:38:27.182 [771631] main/104/interactive/box.load_cfg I> set 'replication' configuration option to []
:3300 | 2025-02-17 23:38:27.182 [771631] main/111/picodata-channels V> cbus endpoint has been created, starting the cbus loop
:3300 | 2025-02-17 23:38:27.190 [771631] main/106/checkpoint_daemon I> scheduled next checkpoint for Tue Feb 18 00:38:31 2025
:3300 | 2025-02-17 23:38:27.201 [771631] main/104/interactive I> tx_binary: bound to 127.0.0.1:3300
:3300 | 2025-02-17 23:38:27.201 [771631] main/104/interactive/box.load_cfg I> set 'listen' configuration option to "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.402 [771631] main/104/interactive I> saving entrypoint StartBoot to pipe 'Fd(22)'
:3300 | 2025-02-17 23:38:27.402 [771631] main/104/interactive I> restarting process to proceed with next entrypoint StartBoot
:3300 | 2025-02-17 23:38:27.402 [771631] main/115/iproto.shutdown I> tx_binary: stopped
:3300 | 2025-02-17 23:38:27.473 [771631] main/104/interactive I> 'cluster.name': "cluster-0-0"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'cluster.default_replication_factor': 3
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.instance_dir': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.service_password_file': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/password.txt"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.cluster_name': "cluster-0-0"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.tier': "default"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.failure_domain': {}
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.iproto_listen': "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.iproto_advertise': "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.peer': ["127.0.0.1:3300"]
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.admin_socket': "/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/admin.sock"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.share_dir': "/home/gmoshkin/code/pd/picodata/test/testplug"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.audit': "/dev/stderr"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.shredding': false
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.log.level': "verbose"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.log.format': "plain"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.memtx.memory': "64M"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.memory': "128M"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.cache': "128M"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.bloom_fpr': 0.05
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.max_tuple_size': "1M"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.page_size': "8K"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.range_size': "1G"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.run_count_per_level': 2
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.run_size_ratio': 3.5
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.read_threads': 1
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.write_threads': 4
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.vinyl.timeout': 60
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.pg.listen': "127.0.0.1:3301"
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> 'instance.pg.ssl': false
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> read entrypoint StartBoot from pipe 'Fd(21)'
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> removing file: /tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000146.xlog
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> removing file: /tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> removing file: /tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.snap
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> entering cluster bootstrap phase
:3300 | 2025-02-17 23:38:27.474 [771631] main/104/interactive I> error injection enabled
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> Tarantool 2.11.5-165-gfae9c5bbe Linux-x86_64-RelWithDebInfo
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> log level 6
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> wal/engine cleanup is paused
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> mapping 67108864 bytes for memtx tuple arena...
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
:3300 | 2025-02-17 23:38:27.475 [771631] main/104/interactive I> mapping 134217728 bytes for vinyl tuple arena...
:3300 | 2025-02-17 23:38:27.477 [771631] main/104/interactive I> update replication_synchro_quorum = 1
:3300 | 2025-02-17 23:38:27.477 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.477 [771631] main/104/interactive I> instance uuid 77fdab61-0002-4cd4-a9a9-c92f78af29e4
:3300 | 2025-02-17 23:38:27.477 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.477 [771631] main/104/interactive I> initializing an empty data directory
:3300 | 2025-02-17 23:38:27.488 [771631] main/104/interactive V> RAFT: recover {term: 1}
:3300 | 2025-02-17 23:38:27.488 [771631] main/104/interactive I> assigned id 1 to replica 77fdab61-0002-4cd4-a9a9-c92f78af29e4
:3300 | 2025-02-17 23:38:27.488 [771631] main/104/interactive I> update replication_synchro_quorum = 1
:3300 | 2025-02-17 23:38:27.488 [771631] main/104/interactive I> cluster uuid b33d17b0-46b7-43a4-9611-98bb29099f4d
:3300 | 2025-02-17 23:38:27.488 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3300 | 2025-02-17 23:38:27.489 [771631] snapshot/101/main I> saving snapshot `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.snap.inprogress'
:3300 | 2025-02-17 23:38:27.491 [771631] snapshot/101/main I> done
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive I> ready to accept requests
:3300 | 2025-02-17 23:38:27.492 [771631] main/105/gc I> wal/engine cleanup is resumed
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive/box.load_cfg I> set 'memtx_memory' configuration option to 67108864
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive/box.load_cfg I> set 'log_level' configuration option to 6
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive/box.load_cfg I> set 'replication' configuration option to []
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive/box.load_cfg I> set 'instance_uuid' configuration option to "77fdab61-0002-4cd4-a9a9-c92f78af29e4"
:3300 | 2025-02-17 23:38:27.492 [771631] main/104/interactive/box.load_cfg I> set 'replicaset_uuid' configuration option to "b33d17b0-46b7-43a4-9611-98bb29099f4d"
:3300 | 2025-02-17 23:38:27.492 [771631] main/111/picodata-channels V> cbus endpoint has been created, starting the cbus loop
:3300 | 2025-02-17 23:38:27.500 [771631] main/106/checkpoint_daemon I> scheduled next checkpoint for Tue Feb 18 01:18:17 2025
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> entering post-join phase
:3300 | {"id":"1.0.1","message":"audit log is ready","severity":"low","time":"2025-02-17T23:38:27.513+0300","title":"init_audit"}
:3300 | {"id":"1.0.2","message":"instance is starting","severity":"low","time":"2025-02-17T23:38:27.513+0300","title":"local_startup"}
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive/box.load_cfg load_cfg.lua:753 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> leaving orphan mode
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive/box.load_cfg I> set 'replication_connect_quorum' configuration option to 0
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive/box.load_cfg I> set 'bootstrap_strategy' configuration option to "legacy"
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {}, learners_next: {}, auto_leave: false }, raft_id: 1
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive V> reset election timeout 0 -> 28 at 0, election_elapsed: 0, timeout: 28, prev_timeout: 0, raft_id: 1
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> became follower at term 1, term: 1, raft_id: 1
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> newRaft, peers: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, last term: 1, last index: 7, applied: 0, commit: 7, term: 1, raft_id: 1
:3300 | 2025-02-17 23:38:27.514 [771631] main/104/interactive I> RawNode created with id 1., id: 1, raft_id: 1
:3300 | 2025-02-17 23:38:27.515 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:27.515 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Replace(_pico_peer_address, [1,"127.0.0.1:3300"]), Insert(_pico_instance, ["default_1_1","77fdab61-0002-4cd4-a9a9-c92f78af29e4",1,"default_1","b33d17b0-46b7-43a4-9611-98bb29099f4d",["Offline",0],["Offline",0],{},"default","24.7.0-1217-ge3d68f09f"]), Insert(_pico_replicaset, ["default_1","b33d17b0-46b7-43a4-9611-98bb29099f4d","default_1_1","default_1_1","default",0.0,"auto","not-ready",0,0,{}])), index: 1
:3300 | {"id":"1.0.3","initiator":"admin","instance_name":"default_1_1","message":"a new instance `default_1_1` joined the cluster","raft_id":"1","severity":"low","time":"2025-02-17T23:38:27.515+0300","title":"join_instance"}
:3300 | {"id":"1.0.4","initiator":"admin","instance_name":"default_1_1","message":"local database created on `default_1_1`","raft_id":"1","severity":"low","time":"2025-02-17T23:38:27.515+0300","title":"create_local_db"}
:3300 | {"id":"1.0.5","initiator":"admin","instance_name":"default_1_1","message":"current state of instance `default_1_1` changed to Offline(0)","new_state":"Offline(0)","raft_id":"1","severity":"medium","time":"2025-02-17T23:38:27.515+0300","title":"change_current_state"}
:3300 | {"id":"1.0.6","initiator":"admin","instance_name":"default_1_1","message":"target state of instance `default_1_1` changed to Offline(0)","new_state":"Offline(0)","raft_id":"1","severity":"low","time":"2025-02-17T23:38:27.515+0300","title":"change_target_state"}
:3300 | 2025-02-17 23:38:27.515 [771631] main/115/governor_loop V> governor_loop_status = #0 'not a leader'
:3300 | 2025-02-17 23:38:27.515 [771631] main/116/sentinel_loop V> waiting until initialized...
:3300 | 2025-02-17 23:38:27.515 [771631] main/104/interactive I> this is the only voter in cluster, triggering election immediately
:3300 | 2025-02-17 23:38:27.515 [771631] main/104/interactive V> can't lock mutex at src/traft/node.rs:429:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:27.515 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Insert(_pico_tier, ["default",3,true,0,0,false])), index: 2
:3300 | 2025-02-17 23:38:27.516 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Insert(_pico_property, ["global_schema_version",0]), Insert(_pico_property, ["next_schema_version",1]), Insert(_pico_property, ["system_catalog_version","25.1.0"]), Insert(_pico_property, ["cluster_version","24.7.0-1217-ge3d68f09f"]), Insert(_pico_db_config, ["auth_password_length_min",8]), Insert(_pico_db_config, ["auth_password_enforce_uppercase",true]), Insert(_pico_db_config, ["auth_password_enforce_lowercase",true]), Insert(_pico_db_config, ["auth_password_enforce_digits",true]), Insert(_pico_db_config, ["auth_password_enforce_specialchars",false]), Insert(_pico_db_config, ["auth_login_attempt_max",4]), Insert(_pico_db_config, ["pg_statement_max",1024]), Insert(_pico_db_config, ["pg_portal_max",1024]), Insert(_pico_db_config, ["raft_snapshot_chunk_size_max",16777216]), Insert(_pico_db_config, ["raft_snapshot_read_view_close_timeout",86400]), Insert(_pico_db_config, ["raft_wal_size_max",67108864]), Insert(_pico_db_config, ["raft_wal_count_max",64]), Insert(_pico_db_config, ["governor_auto_offline_timeout",30.0]), Insert(_pico_db_config, ["governor_raft_op_timeout",3.0]), Insert(_pico_db_config, ["governor_common_rpc_timeout",3.0]), Insert(_pico_db_config, ["governor_plugin_rpc_timeout",10.0]), Insert(_pico_db_config, ["sql_vdbe_opcode_max",45000]), Insert(_pico_db_config, ["sql_motion_row_max",5000]), Insert(_pico_db_config, ["memtx_checkpoint_count",2]), Insert(_pico_db_config, ["memtx_checkpoint_interval",3600]), Insert(_pico_db_config, ["iproto_net_msg_max",768])), index: 3
:3300 | 2025-02-17 23:38:27.516 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Insert(_pico_user, [0,"guest",0,["md5","md5084e0343a0486ff05530df6c705c8bb4"],1,"user"]), Insert(_pico_privilege, [1,0,"login","universe",0,0]), Insert(_pico_privilege, [1,0,"execute","role",2,0]), Insert(_pico_user, [1,"admin",0,["md5",""],1,"user"]), Insert(_pico_privilege, [1,1,"read","universe",0,0]), Insert(_pico_privilege, [1,1,"write","universe",0,0]), Insert(_pico_privilege, [1,1,"execute","universe",0,0]), Insert(_pico_privilege, [1,1,"login","universe",0,0]), Insert(_pico_privilege, [1,1,"create","universe",0,0]), Insert(_pico_privilege, [1,1,"drop","universe",0,0]), Insert(_pico_privilege, [1,1,"alter","universe",0,0]), Insert(_pico_user, [32,"pico_service",0,["chap-sha1","JHDAwG3uQv0WGLuZAFrcouydHhk="],1,"user"]), Insert(_pico_privilege, [1,32,"read","universe",0,0]), Insert(_pico_privilege, [1,32,"write","universe",0,0]), Insert(_pico_privilege, [1,32,"execute","universe",0,0]), Insert(_pico_privilege, [1,32,"login","universe",0,0]), Insert(_pico_privilege, [1,32,"create","universe",0,0]), Insert(_pico_privilege, [1,32,"drop","universe",0,0]), Insert(_pico_privilege, [1,32,"alter","universe",0,0]), Insert(_pico_privilege, [1,32,"execute","role",3,0]), Insert(_pico_user, [2,"public",0,null,1,"role"]), Insert(_pico_user, [31,"super",0,null,1,"role"]), Insert(_pico_user, [3,"replication",0,null,1,"role"])), index: 4
:3300 | 2025-02-17 23:38:27.516 [771631] main/114/raft_main_loop V> applying entry: ChangeAuth(1, 0, 1), index: 5
:3300 | {"auth_type":"md5","id":"1.0.7","initiator":"admin","message":"password of user `guest` was changed","severity":"high","time":"2025-02-17T23:38:27.516+0300","title":"change_password","user":"guest"}
:3300 | 2025-02-17 23:38:27.518 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Insert(_pico_table, [512,"_pico_table",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"id"},{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"map","is_nullable":false,"name":"distribution"},{"field_type":"array","is_nullable":false,"name":"format"},{"field_type":"unsigned","is_nullable":false,"name":"schema_version"},{"field_type":"boolean","is_nullable":false,"name":"operable"},{"field_type":"string","is_nullable":false,"name":"engine"},{"field_type":"unsigned","is_nullable":false,"name":"owner"},{"field_type":"string","is_nullable":false,"name":"description"}],0,true,"memtx",1,"Stores metadata of all the cluster tables in picodata."]), Insert(_pico_index, [512,0,"_pico_table_id","tree",[{"unique":true}],[["id","unsigned",null,false,null]],true,0]), Insert(_pico_index, [512,1,"_pico_table_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_index, [512,2,"_pico_table_owner_id","tree",[{"unique":false}],[["owner","unsigned",null,false,null]],true,0]), Insert(_pico_table, [513,"_pico_index",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"table_id"},{"field_type":"unsigned","is_nullable":false,"name":"id"},{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"string","is_nullable":false,"name":"type"},{"field_type":"array","is_nullable":false,"name":"opts"},{"field_type":"array","is_nullable":false,"name":"parts"},{"field_type":"boolean","is_nullable":false,"name":"operable"},{"field_type":"unsigned","is_nullable":false,"name":"schema_version"}],0,true,"memtx",1,""]), Insert(_pico_index, [513,0,"_pico_index_id","tree",[{"unique":true}],[["table_id","unsigned",null,false,null],["id","unsigned",null,false,null]],true,0]), Insert(_pico_index, [513,1,"_pico_index_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_table, [514,"_pico_peer_address",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"raft_id"},{"field_type":"string","is_nullable":false,"name":"address"}],0,true,"memtx",1,""]), Insert(_pico_index, [514,0,"_pico_peer_address_raft_id","tree",[{"unique":true}],[["raft_id","unsigned",null,false,null]],true,0]), Insert(_pico_table, [515,"_pico_instance",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"string","is_nullable":false,"name":"uuid"},{"field_type":"unsigned","is_nullable":false,"name":"raft_id"},{"field_type":"string","is_nullable":false,"name":"replicaset_name"},{"field_type":"string","is_nullable":false,"name":"replicaset_uuid"},{"field_type":"array","is_nullable":false,"name":"current_state"},{"field_type":"array","is_nullable":false,"name":"target_state"},{"field_type":"map","is_nullable":false,"name":"failure_domain"},{"field_type":"string","is_nullable":false,"name":"tier"},{"field_type":"string","is_nullable":false,"name":"picodata_version"}],0,true,"memtx",1,""]), Insert(_pico_index, [515,0,"_pico_instance_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_index, [515,1,"_pico_instance_uuid","tree",[{"unique":true}],[["uuid","string",null,false,null]],true,0]), Insert(_pico_index, [515,2,"_pico_instance_raft_id","tree",[{"unique":true}],[["raft_id","unsigned",null,false,null]],true,0]), Insert(_pico_index, [515,3,"_pico_instance_replicaset_name","tree",[{"unique":false}],[["replicaset_name","string",null,false,null]],true,0]), Insert(_pico_table, [516,"_pico_property",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"key"},{"field_type":"any","is_nullable":false,"name":"value"}],0,true,"memtx",1,""]), Insert(_pico_index, [516,0,"_pico_property_key","tree",[{"unique":true}],[["key","string",null,false,null]],true,0]), Insert(_pico_table, [517,"_pico_replicaset",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"string","is_nullable":false,"name":"uuid"},{"field_type":"string","is_nullable":false,"name":"current_master_name"},{"field_type":"string","is_nullable":false,"name":"target_master_name"},{"field_type":"string","is_nullable":false,"name":"tier"},{"field_type":"double","is_nullable":false,"name":"weight"},{"field_type":"string","is_nullable":false,"name":"weight_origin"},{"field_type":"string","is_nullable":false,"name":"state"},{"field_type":"unsigned","is_nullable":false,"name":"current_config_version"},{"field_type":"unsigned","is_nullable":false,"name":"target_config_version"},{"field_type":"map","is_nullable":false,"name":"promotion_vclock"}],0,true,"memtx",1,""]), Insert(_pico_index, [517,0,"_pico_replicaset_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_index, [517,1,"_pico_replicaset_uuid","tree",[{"unique":true}],[["uuid","string",null,false,null]],true,0]), Insert(_pico_table, [520,"_pico_user",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"id"},{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"unsigned","is_nullable":false,"name":"schema_version"},{"field_type":"array","is_nullable":true,"name":"auth"},{"field_type":"unsigned","is_nullable":false,"name":"owner"},{"field_type":"string","is_nullable":false,"name":"type"}],0,true,"memtx",1,""]), Insert(_pico_index, [520,0,"_pico_user_id","tree",[{"unique":true}],[["id","unsigned",null,false,null]],true,0]), Insert(_pico_index, [520,1,"_pico_user_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_index, [520,2,"_pico_user_owner_id","tree",[{"unique":false}],[["owner","unsigned",null,false,null]],true,0]), Insert(_pico_table, [521,"_pico_privilege",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"grantor_id"},{"field_type":"unsigned","is_nullable":false,"name":"grantee_id"},{"field_type":"string","is_nullable":false,"name":"privilege"},{"field_type":"string","is_nullable":false,"name":"object_type"},{"field_type":"integer","is_nullable":false,"name":"object_id"},{"field_type":"unsigned","is_nullable":false,"name":"schema_version"}],0,true,"memtx",1,""]), Insert(_pico_index, [521,0,"_pico_privilege_primary","tree",[{"unique":true}],[["grantee_id","unsigned",null,false,null],["object_type","string",null,false,null],["object_id","integer",null,false,null],["privilege","string",null,false,null]],true,0]), Insert(_pico_index, [521,1,"_pico_privilege_object","tree",[{"unique":false}],[["object_type","string",null,false,null],["object_id","integer",null,false,null]],true,0]), Insert(_pico_table, [523,"_pico_tier",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"unsigned","is_nullable":false,"name":"replication_factor"},{"field_type":"boolean","is_nullable":false,"name":"can_vote"},{"field_type":"unsigned","is_nullable":false,"name":"current_vshard_config_version"},{"field_type":"unsigned","is_nullable":false,"name":"target_vshard_config_version"},{"field_type":"boolean","is_nullable":false,"name":"vshard_bootstrapped"}],0,true,"memtx",1,""]), Insert(_pico_index, [523,0,"_pico_tier_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_table, [524,"_pico_routine",{"Global":null},[{"field_type":"unsigned","is_nullable":false,"name":"id"},{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"string","is_nullable":false,"name":"kind"},{"field_type":"array","is_nullable":false,"name":"params"},{"field_type":"array","is_nullable":false,"name":"returns"},{"field_type":"string","is_nullable":false,"name":"language"},{"field_type":"string","is_nullable":false,"name":"body"},{"field_type":"string","is_nullable":false,"name":"security"},{"field_type":"boolean","is_nullable":false,"name":"operable"},{"field_type":"unsigned","is_nullable":false,"name":"schema_version"},{"field_type":"unsigned","is_nullable":false,"name":"owner"}],0,true,"memtx",1,""]), Insert(_pico_index, [524,0,"_pico_routine_id","tree",[{"unique":true}],[["id","unsigned",null,false,null]],true,0]), Insert(_pico_index, [524,1,"_pico_routine_name","tree",[{"unique":true}],[["name","string",null,false,null]],true,0]), Insert(_pico_index, [524,2,"_pico_routine_owner_id","tree",[{"unique":false}],[["owner","unsigned",null,false,null]],true,0]), Insert(_pico_table, [526,"_pico_plugin",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"boolean","is_nullable":false,"name":"enabled"},{"field_type":"array","is_nullable":false,"name":"services"},{"field_type":"string","is_nullable":false,"name":"version"},{"field_type":"string","is_nullable":false,"name":"description"},{"field_type":"array","is_nullable":false,"name":"migration_list"}],0,true,"memtx",1,""]), Insert(_pico_index, [526,0,"_pico_plugin_name","tree",[{"unique":true}],[["name","string",null,false,null],["version","string",null,false,null]],true,0]), Insert(_pico_table, [527,"_pico_service",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"plugin_name"},{"field_type":"string","is_nullable":false,"name":"name"},{"field_type":"string","is_nullable":false,"name":"version"},{"field_type":"array","is_nullable":false,"name":"tiers"},{"field_type":"string","is_nullable":false,"name":"description"}],0,true,"memtx",1,""]), Insert(_pico_index, [527,0,"_pico_service_name","tree",[{"unique":true}],[["plugin_name","string",null,false,null],["name","string",null,false,null],["version","string",null,false,null]],true,0]), Insert(_pico_table, [528,"_pico_service_route",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"plugin_name"},{"field_type":"string","is_nullable":false,"name":"plugin_version"},{"field_type":"string","is_nullable":false,"name":"service_name"},{"field_type":"string","is_nullable":false,"name":"instance_name"},{"field_type":"boolean","is_nullable":false,"name":"poison"}],0,true,"memtx",1,""]), Insert(_pico_index, [528,0,"_pico_service_routing_key","tree",[{"unique":true}],[["plugin_name","string",null,false,null],["plugin_version","string",null,false,null],["service_name","string",null,false,null],["instance_name","string",null,false,null]],true,0]), Insert(_pico_table, [529,"_pico_plugin_migration",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"plugin_name"},{"field_type":"string","is_nullable":false,"name":"migration_file"},{"field_type":"string","is_nullable":false,"name":"hash"}],0,true,"memtx",1,""]), Insert(_pico_index, [529,0,"_pico_plugin_migration_primary_key","tree",[{"unique":true}],[["plugin_name","string",null,false,null],["migration_file","string",null,false,null]],true,0]), Insert(_pico_table, [530,"_pico_plugin_config",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"plugin"},{"field_type":"string","is_nullable":false,"name":"version"},{"field_type":"string","is_nullable":false,"name":"entity"},{"field_type":"string","is_nullable":false,"name":"key"},{"field_type":"any","is_nullable":true,"name":"value"}],0,true,"memtx",1,""]), Insert(_pico_index, [530,0,"_pico_plugin_config_pk","tree",[{"unique":true}],[["plugin","string",null,false,null],["version","string",null,false,null],["entity","string",null,false,null],["key","string",null,false,null]],true,0]), Insert(_pico_table, [531,"_pico_db_config",{"Global":null},[{"field_type":"string","is_nullable":false,"name":"key"},{"field_type":"any","is_nullable":false,"name":"value"}],0,true,"memtx",1,""]), Insert(_pico_index, [531,0,"_pico_db_config_key","tree",[{"unique":true}],[["key","string",null,false,null]],true,0])), index: 6
:3300 | 2025-02-17 23:38:27.518 [771631] main/114/raft_main_loop I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {}, learners_next: {}, auto_leave: false }, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive I> starting a new election, term: 1, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive I> became pre-candidate at term 1, term: 1, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive V> reset election timeout 28 -> 29 at 2, election_elapsed: 2, timeout: 29, prev_timeout: 28, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive I> became candidate at term 2, term: 2, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive V> reset election timeout 29 -> 31 at 0, election_elapsed: 0, timeout: 31, prev_timeout: 29, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/104/interactive I> became leader at term 2, term: 2, raft_id: 1
:3300 | 2025-02-17 23:38:27.518 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:27.518 [771631] main/114/raft_main_loop V> hard state: term: 2 vote: 1 commit: 7
:3300 | 2025-02-17 23:38:27.519 [771631] main/115/governor_loop plan.rs:89 W> leader is going offline and no substitution is found, voters: [1], leader_raft_id: 1
:3300 | 2025-02-17 23:38:27.519 [771631] main/115/governor_loop I> target master default_1_1 of replicaset default_1 is going Offline: trying to choose a new one
:3300 | 2025-02-17 23:38:27.519 [771631] main/115/governor_loop plan.rs:1345 W> there are no instances suitable as master of replicaset default_1
:3300 | 2025-02-17 23:38:27.519 [771631] main/115/governor_loop V> governor_loop_status = #0 'idle'
:3300 | 2025-02-17 23:38:27.519 [771631] main/115/governor_loop I> nothing to do, waiting for events to handle
:3300 | 2025-02-17 23:38:27.519 [771631] main/104/interactive I> tx_binary: bound to 127.0.0.1:3300
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> persisted index 8, raft_id: 1
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> committing index 8, index: 8, raft_id: 1
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> commit index: 8
:3300 | 2025-02-17 23:38:27.519 [771631] main/104/interactive/box.load_cfg I> set 'listen' configuration option to "127.0.0.1:3300"
:3300 | 2025-02-17 23:38:27.519 [771631] main/117/console/unix/:/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/admin.sock/socket I> started
:3300 | 2025-02-17 23:38:27.519 [771631] main/104/interactive I> initiating self-activation of default_1_1
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> applying entry: Nop, index: 8
:3300 | 2025-02-17 23:38:27.519 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:27.520 [771631] main/110/.proc_update_instance V> got update instance request: Request { instance_name: InstanceName("default_1_1"), cluster_name: "cluster-0-0", current_state: None, target_state: Some(Online), failure_domain: Some(FailureDomain), dont_retry: false, picodata_version: Some("24.7.0-1217-ge3d68f09f") }
:3300 | 2025-02-17 23:38:27.520 [771631] main/110/.proc_update_instance update_instance.rs:278 E> joinee version is 24.7.0-1217-ge3d68f09f
:3300 | 2025-02-17 23:38:27.520 [771631] main/110/.proc_update_instance V> waiting for applied index 9
:3300 | 2025-02-17 23:38:27.620 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:27.620 [771631] main/114/raft_main_loop V> persisted index 9, raft_id: 1
:3300 | 2025-02-17 23:38:27.621 [771631] main/114/raft_main_loop V> committing index 9, index: 9, raft_id: 1
:3300 | 2025-02-17 23:38:27.621 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3300 | 2025-02-17 23:38:27.621 [771631] main/114/raft_main_loop V> commit index: 9
:3300 | 2025-02-17 23:38:27.621 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:27.621 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Update(_pico_instance, ["default_1_1"], [["=","target_state",["Online",1]]]), Update(_pico_replicaset, ["default_1"], [["=","target_config_version",1]]), Update(_pico_tier, ["default"], [["=","target_vshard_config_version",1]])), index: 9
:3300 | {"id":"1.0.8","initiator":"admin","instance_name":"default_1_1","message":"target state of instance `default_1_1` changed to Online(1)","new_state":"Online(1)","raft_id":"1","severity":"low","time":"2025-02-17T23:38:27.621+0300","title":"change_target_state"}
:3300 | 2025-02-17 23:38:27.623 [771631] main/115/governor_loop V> governor_loop_status = #1 'configure replication'
:3300 | 2025-02-17 23:38:27.623 [771631] main/115/governor_loop I> configuring replication
:3300 | 2025-02-17 23:38:27.623 [771631] main/115/governor_loop I> calling proc_replication, is_master: true, instance_name: default_1_1
:3300 | 2025-02-17 23:38:27.623 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:27.623 [771631] main/110/.proc_update_instance V> done waiting for applied index 9, current: 9
:3300 | 2025-02-17 23:38:27.624 [771631] main/104/interactive I> self-activated successfully
:3300 | 2025-02-17 23:38:27.624 [771631] main/104/interactive I> starting postgres server at ("127.0.0.1", 3301)...
:3300 | 2025-02-17 23:38:27.625 [771631] main/123/pgproto I> creating statement storage with capacity 1024
:3300 | 2025-02-17 23:38:27.625 [771631] main/123/pgproto I> creating portal storage with capacity 1024
:3300 | {"id":"1.0.9","initiator":"admin","instance_name":"default_1_1","message":"local database connected on `default_1_1`","raft_id":"1","severity":"low","time":"2025-02-17T23:38:27.624+0300","title":"connect_local_db"}
:3300 | 2025-02-17 23:38:27.625 [771631] main/112/<unknown> V> PG socket bind result: Ok(TcpListener { addr: 127.0.0.1:3301, fd: 25 })
:3300 | 2025-02-17 23:38:27.625 [771631] main/104/interactive I> skipped luaL_loadfile due to picodata callback
:3300 | 2025-02-17 23:38:27.625 [771631] main I> entering the event loop
:3300 | 2025-02-17 23:38:27.626 [771631] main/110/.proc_replication I> connecting to 1 replicas
:3300 | 2025-02-17 23:38:27.626 [771631] main/110/.proc_replication C> failed to connect to 1 out of 1 replicas
:3300 | 2025-02-17 23:38:27.627 [771631] main/110/.proc_replication I> leaving orphan mode
:3300 | 2025-02-17 23:38:27.627 [771631] main/110/.proc_replication/box.load_cfg I> set 'replication' configuration option to ["pico_service@127.0.0.1:3300"]
:3300 | 2025-02-17 23:38:27.627 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_1
:3300 | 2025-02-17 23:38:27.627 [771631] main/115/governor_loop I> actualizing replicaset configuration version, replicaset_name: default_1
:3300 | 2025-02-17 23:38:27.628 [771631] main/115/governor_loop V> waiting for applied index 10
:3300 | 2025-02-17 23:38:27.628 [771631] main/125/applier/pico_service@127.0.0.1:3300 I> remote master 77fdab61-0002-4cd4-a9a9-c92f78af29e4 at 127.0.0.1:3300 running Tarantool 2.11.5
:3300 | 2025-02-17 23:38:27.628 [771631] main/125/applier/pico_service@127.0.0.1:3300 I> leaving orphan mode
:3300 | 2025-02-17 23:38:27.724 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:27.724 [771631] main/114/raft_main_loop V> persisted index 10, raft_id: 1
:3300 | 2025-02-17 23:38:27.725 [771631] main/114/raft_main_loop V> committing index 10, index: 10, raft_id: 1
:3300 | 2025-02-17 23:38:27.725 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3300 | 2025-02-17 23:38:27.725 [771631] main/114/raft_main_loop V> commit index: 10
:3300 | 2025-02-17 23:38:27.725 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:27.725 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_replicaset, ["default_1"], [["=","current_config_version",1]]), index: 10
:3300 | 2025-02-17 23:38:27.726 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:27.726 [771631] main/115/governor_loop V> done waiting for applied index 10, current: 10
:3300 | 2025-02-17 23:38:27.727 [771631] main/115/governor_loop V> governor_loop_status = #2 'update instance state to online'
:3300 | 2025-02-17 23:38:27.727 [771631] main/115/governor_loop I> enable plugins on instance, instance_name: default_1_1
:3300 | 2025-02-17 23:38:27.727 [771631] main/110/.proc_enable_all_plugins V> waiting for applied index 10
:3300 | 2025-02-17 23:38:27.727 [771631] main/110/.proc_enable_all_plugins V> done waiting for applied index 10, current: 10
:3300 | 2025-02-17 23:38:27.728 [771631] main/115/governor_loop I> handling instance state change, current_state: Online, instance_name: default_1_1
:3300 | 2025-02-17 23:38:27.728 [771631] main/115/governor_loop V> waiting for applied index 11
:3300 | 2025-02-17 23:38:27.826 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:27.827 [771631] main/114/raft_main_loop V> persisted index 11, raft_id: 1
:3300 | 2025-02-17 23:38:27.827 [771631] main/114/raft_main_loop V> committing index 11, index: 11, raft_id: 1
:3300 | 2025-02-17 23:38:27.827 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3300 | 2025-02-17 23:38:27.827 [771631] main/114/raft_main_loop V> commit index: 11
:3300 | 2025-02-17 23:38:27.827 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:27.828 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_instance, ["default_1_1"], [["=","current_state",["Online",1]]]), index: 11
:3300 | {"id":"1.0.10","initiator":"admin","instance_name":"default_1_1","message":"current state of instance `default_1_1` changed to Online(1)","new_state":"Online(1)","raft_id":"1","severity":"medium","time":"2025-02-17T23:38:27.827+0300","title":"change_current_state"}
:3301 | 2025-02-17 23:38:27.828 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:27.828 [771631] main/115/governor_loop V> done waiting for applied index 11, current: 11
:3301 | 2025-02-17 23:38:27.829 [771631] main/115/governor_loop V> governor_loop_status = #3 'idle'
:3301 | 2025-02-17 23:38:27.829 [771631] main/115/governor_loop I> nothing to do, waiting for events to handle
:3301 | 2025-02-17 23:38:28.424 [771631] main/110/.proc_raft_join V> waiting for applied index 12
:3301 | 2025-02-17 23:38:28.425 [771631] main/128/.proc_raft_join V> can't lock mutex at src/rpc/join.rs:104:39, already locked at src/rpc/join.rs:104:39
:3301 | 2025-02-17 23:38:28.431 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3301 | 2025-02-17 23:38:28.431 [771631] main/114/raft_main_loop V> persisted index 12, raft_id: 1
:3301 | 2025-02-17 23:38:28.431 [771631] main/114/raft_main_loop V> committing index 12, index: 12, raft_id: 1
:3301 | 2025-02-17 23:38:28.432 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3301 | 2025-02-17 23:38:28.432 [771631] main/114/raft_main_loop V> commit index: 12
:3301 | 2025-02-17 23:38:28.432 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3301 | 2025-02-17 23:38:28.432 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Replace(_pico_peer_address, [2,"127.0.0.1:3302"]), Replace(_pico_instance, ["default_1_2","caf0b1e6-ae28-4955-9c34-35169d154b1a",2,"default_1","b33d17b0-46b7-43a4-9611-98bb29099f4d",["Offline",0],["Offline",0],{},"default","24.7.0-1217-ge3d68f09f"])), index: 12
:3301 | {"id":"1.0.11","initiator":"admin","instance_name":"default_1_2","message":"a new instance `default_1_2` joined the cluster","raft_id":"2","severity":"low","time":"2025-02-17T23:38:28.432+0300","title":"join_instance"}
:3301 | {"id":"1.0.12","initiator":"admin","instance_name":"default_1_2","message":"local database created on `default_1_2`","raft_id":"2","severity":"low","time":"2025-02-17T23:38:28.432+0300","title":"create_local_db"}
:3301 | {"id":"1.0.13","initiator":"admin","instance_name":"default_1_2","message":"current state of instance `default_1_2` changed to Offline(0)","new_state":"Offline(0)","raft_id":"2","severity":"medium","time":"2025-02-17T23:38:28.432+0300","title":"change_current_state"}
:3301 | {"id":"1.0.14","initiator":"admin","instance_name":"default_1_2","message":"target state of instance `default_1_2` changed to Offline(0)","new_state":"Offline(0)","raft_id":"2","severity":"low","time":"2025-02-17T23:38:28.432+0300","title":"change_target_state"}
:3301 | 2025-02-17 23:38:28.434 [771631] main/115/governor_loop V> governor_loop_status = #4 'conf change'
:3301 | 2025-02-17 23:38:28.434 [771631] main/115/governor_loop I> proposing conf_change, cc: changes {change_type: AddLearnerNode node_id: 2}
:3301 | 2025-02-17 23:38:28.434 [771631] main/115/governor_loop V> can't lock mutex at src/traft/node.rs:444:18, already locked at src/traft/node.rs:2570:45
:3301 | 2025-02-17 23:38:28.434 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:28.434 [771631] main/110/.proc_raft_join V> done waiting for applied index 12, current: 12
:3301 | 2025-02-17 23:38:28.435 [771631] main/110/.proc_raft_join I> new instance joined the cluster: Instance { name: InstanceName("default_1_2"), uuid: "caf0b1e6-ae28-4955-9c34-35169d154b1a", raft_id: 2, replicaset_name: ReplicasetName("default_1"), replicaset_uuid: "b33d17b0-46b7-43a4-9611-98bb29099f4d", current_state: State { variant: Offline, incarnation: 0 }, target_state: State { variant: Offline, incarnation: 0 }, failure_domain: FailureDomain, tier: "default", picodata_version: "24.7.0-1217-ge3d68f09f" }
:3301 | 2025-02-17 23:38:28.435 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3301 | 2025-02-17 23:38:28.436 [771631] main/128/.proc_raft_join V> can't lock mutex at src/traft/node.rs:310:24, already locked at src/traft/node.rs:2570:45
:3301 | 2025-02-17 23:38:28.436 [771631] main/114/raft_main_loop V> persisted index 13, raft_id: 1
:3301 | 2025-02-17 23:38:28.436 [771631] main/114/raft_main_loop V> committing index 13, index: 13, raft_id: 1
:3301 | 2025-02-17 23:38:28.436 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3301 | 2025-02-17 23:38:28.436 [771631] main/114/raft_main_loop V> commit index: 13
:3301 | 2025-02-17 23:38:28.437 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3301 | 2025-02-17 23:38:28.437 [771631] main/114/raft_main_loop I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {2}, learners_next: {}, auto_leave: false }, raft_id: 1
:3301 | 2025-02-17 23:38:28.437 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 12 entries {entry_type: EntryConfChangeV2 term: 2 index: 13 data: "\022\004\010\002\020\002"} commit: 13, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.437 [771631] main/115/governor_loop V> governor_loop_status = #5 'idle'
:3301 | 2025-02-17 23:38:28.437 [771631] main/115/governor_loop I> nothing to do, waiting for events to handle
:3301 | 2025-02-17 23:38:28.437 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:28.438 [771631] main/128/.proc_raft_join V> waiting for applied index 14
:3301 | 2025-02-17 23:38:28.441 [771631] main/110/main I> joining replica caf0b1e6-ae28-4955-9c34-35169d154b1a at fd 26, aka 127.0.0.1:3300, peer of 127.0.0.1:40480
:3301 | 2025-02-17 23:38:28.442 [771631] main/110/main I> initial data sent.
:3301 | 2025-02-17 23:38:28.442 [771631] main/110/main I> assigned id 2 to replica caf0b1e6-ae28-4955-9c34-35169d154b1a
:3301 | 2025-02-17 23:38:28.442 [771631] main I> update replication_synchro_quorum = 2
:3301 | 2025-02-17 23:38:28.442 [771631] main I> RAFT: fencing paused
:3301 | 2025-02-17 23:38:28.442 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3301 | 2025-02-17 23:38:28.442 [771631] relay/127.0.0.1:40480/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3301 | 2025-02-17 23:38:28.443 [771631] main/110/main I> final data sent.
:3301 | 2025-02-17 23:38:28.461 [771631] main/110/main I> subscribed replica caf0b1e6-ae28-4955-9c34-35169d154b1a at fd 26, aka 127.0.0.1:3300, peer of 127.0.0.1:40480
:3301 | 2025-02-17 23:38:28.461 [771631] main/110/main I> remote vclock {1: 149} local vclock {0: 164, 1: 149}
:3301 | 2025-02-17 23:38:28.462 [771631] relay/127.0.0.1:40480/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3301 | 2025-02-17 23:38:28.539 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgHeartbeat to: 2, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.539 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3301 | 2025-02-17 23:38:28.539 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3301 | 2025-02-17 23:38:28.539 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3301 | 2025-02-17 23:38:28.540 [771631] main/114/raft_main_loop V> persisted index 14, raft_id: 1
:3301 | 2025-02-17 23:38:28.540 [771631] main/114/raft_main_loop V> committing index 14, index: 14, raft_id: 1
:3301 | 2025-02-17 23:38:28.540 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3301 | 2025-02-17 23:38:28.540 [771631] main/114/raft_main_loop V> commit index: 14
:3301 | 2025-02-17 23:38:28.541 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3301 | 2025-02-17 23:38:28.541 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Replace(_pico_peer_address, [3,"127.0.0.1:3304"]), Replace(_pico_instance, ["default_1_3","35662ba5-d645-45bb-89b6-0a4553e26627",3,"default_1","b33d17b0-46b7-43a4-9611-98bb29099f4d",["Offline",0],["Offline",0],{},"default","24.7.0-1217-ge3d68f09f"])), index: 14
:3301 | {"id":"1.0.15","initiator":"admin","instance_name":"default_1_3","message":"a new instance `default_1_3` joined the cluster","raft_id":"3","severity":"low","time":"2025-02-17T23:38:28.540+0300","title":"join_instance"}
:3301 | {"id":"1.0.16","initiator":"admin","instance_name":"default_1_3","message":"local database created on `default_1_3`","raft_id":"3","severity":"low","time":"2025-02-17T23:38:28.541+0300","title":"create_local_db"}
:3301 | {"id":"1.0.17","initiator":"admin","instance_name":"default_1_3","message":"current state of instance `default_1_3` changed to Offline(0)","new_state":"Offline(0)","raft_id":"3","severity":"medium","time":"2025-02-17T23:38:28.541+0300","title":"change_current_state"}
:3301 | {"id":"1.0.18","initiator":"admin","instance_name":"default_1_3","message":"target state of instance `default_1_3` changed to Offline(0)","new_state":"Offline(0)","raft_id":"3","severity":"low","time":"2025-02-17T23:38:28.541+0300","title":"change_target_state"}
:3301 | 2025-02-17 23:38:28.542 [771631] main/115/governor_loop V> governor_loop_status = #6 'conf change'
:3301 | 2025-02-17 23:38:28.542 [771631] main/115/governor_loop I> proposing conf_change, cc: changes {change_type: AddLearnerNode node_id: 3}
:3301 | 2025-02-17 23:38:28.542 [771631] main/115/governor_loop V> can't lock mutex at src/traft/node.rs:444:18, already locked at src/traft/node.rs:2570:45
:3301 | 2025-02-17 23:38:28.543 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:28.543 [771631] main/128/.proc_raft_join V> done waiting for applied index 14, current: 14
:3301 | 2025-02-17 23:38:28.543 [771631] main/128/.proc_raft_join I> new instance joined the cluster: Instance { name: InstanceName("default_1_3"), uuid: "35662ba5-d645-45bb-89b6-0a4553e26627", raft_id: 3, replicaset_name: ReplicasetName("default_1"), replicaset_uuid: "b33d17b0-46b7-43a4-9611-98bb29099f4d", current_state: State { variant: Offline, incarnation: 0 }, target_state: State { variant: Offline, incarnation: 0 }, failure_domain: FailureDomain, tier: "default", picodata_version: "24.7.0-1217-ge3d68f09f" }
:3301 | 2025-02-17 23:38:28.543 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3301 | 2025-02-17 23:38:28.543 [771631] main/114/raft_main_loop V> persisted index 15, raft_id: 1
:3301 | 2025-02-17 23:38:28.544 [771631] main/114/raft_main_loop V> committing index 15, index: 15, raft_id: 1
:3301 | 2025-02-17 23:38:28.544 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3301 | 2025-02-17 23:38:28.544 [771631] main/114/raft_main_loop V> commit index: 15
:3301 | 2025-02-17 23:38:28.544 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3301 | 2025-02-17 23:38:28.544 [771631] main/114/raft_main_loop I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {2, 3}, learners_next: {}, auto_leave: false }, raft_id: 1
:3301 | 2025-02-17 23:38:28.545 [771631] main/114/raft_main_loop V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 14 entries {entry_type: EntryConfChangeV2 term: 2 index: 15 data: "\022\004\010\002\020\003"} commit: 15, to: 3, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.545 [771631] main/115/governor_loop V> governor_loop_status = #7 'idle'
:3301 | 2025-02-17 23:38:28.545 [771631] main/115/governor_loop I> nothing to do, waiting for events to handle
:3301 | 2025-02-17 23:38:28.546 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:28.549 [771631] main/128/.proc_raft_interact V> received msgAppend rejection, index: 12, from: 2, reject_hint_term: 0, reject_hint_index: 0, raft_id: 1
:3301 | 2025-02-17 23:38:28.549 [771631] main/128/.proc_raft_interact V> decreased progress of 2, progress: Progress { matched: 0, next_idx: 1, state: Probe, paused: false, pending_snapshot: 0, pending_request_snapshot: 0, recent_active: true, ins: Inflights { start: 0, count: 0, buffer: [], cap: 256, incoming_cap: None }, commit_group_id: 0, committed_index: 0 }, raft_id: 1
:3301 | 2025-02-17 23:38:28.550 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 entries {term: 1 index: 1 context: "\222\251batch_dml\223\224\247replace\315\002\002\304\021\222\001\256127.0.0.1:3300\001\224\246insert\315\002\003\304\230\232\253default_1_1\331$77fdab61-0002-4cd4-a9a9-c92f78af29e4\001\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001\224\246insert\315\002\005\304l\233\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\253default_1_1\253default_1_1\247default\313\000\000\000\000\000\000\000\000\244auto\251not-ready\000\000\200\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.551 [771631] main/129/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 entries {term: 1 index: 1 context: "\222\251batch_dml\223\224\247replace\315\002\002\304\021\222\001\256127.0.0.1:3300\001\224\246insert\315\002\003\304\230\232\253default_1_1\331$77fdab61-0002-4cd4-a9a9-c92f78af29e4\001\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001\224\246insert\315\002\005\304l\233\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\253default_1_1\253default_1_1\247default\313\000\000\000\000\000\000\000\000\244auto\251not-ready\000\000\200\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.551 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3301 | 2025-02-17 23:38:28.551 [771631] main/114/raft_main_loop V> done sending messages, sent: 3, skipped: 0
:3301 | 2025-02-17 23:38:28.551 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3301 | 2025-02-17 23:38:28.551 [771631] main/133/to:default_1_3 network.rs:273 W> error when sending message to peer: server responded with error: Loading: Instance bootstrap hasn't finished yet, raft_id: 3
:3301 | 2025-02-17 23:38:28.551 [771631] main/133/to:default_1_3 V> reconnecting to 127.0.0.1:3304, client_version: 1
:3301 | 2025-02-17 23:38:28.552 [771631] main/129/main I> joining replica 35662ba5-d645-45bb-89b6-0a4553e26627 at fd 34, aka 127.0.0.1:3300, peer of 127.0.0.1:40498
:3301 | 2025-02-17 23:38:28.553 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 1 entries {term: 1 index: 2 context: "\222\251batch_dml\221\224\246insert\315\002\013\304\016\226\247default\003\303\000\000\302\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.554 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 2 entries {term: 1 index: 3 context: "\222\251batch_dml\334\000\031\224\246insert\315\002\004\304\030\222\265global_schema_version\000\001\224\246insert\315\002\004\304\026\222\263next_schema_version\001\001\224\246insert\315\002\004\304\037\222\266system_catalog_version\24625.1.0\001\224\246insert\315\002\004\304(\222\257cluster_version\26624.7.0-1217-ge3d68f09f\001\224\246insert\315\002\023\304\033\222\270auth_password_length_min\010\001\224\246insert\315\002\023\304\"\222\277auth_password_enforce_uppercase\303\001\224\246insert\315\002\023\304\"\222\277auth_password_enforce_lowercase\303\001\224\246insert\315\002\023\304\037\222\274auth_password_enforce_digits\303\001\224\246insert\315\002\023\304&\222\331\"auth_password_enforce_specialchars\302\001\224\246insert\315\002\023\304\031\222\266auth_login_attempt_max\004\001\224\246insert\315\002\023\304\025\222\260pg_statement_max\315\004\000\001\224\246insert\315\002\023\304\022\222\255pg_portal_max\315\004\000\001\224\246insert\315\002\023\304#\222\274raft_snapshot_chunk_size_max\316\001\000\000\000\001\224\246insert\315\002\023\304-\222\331%raft_snapshot_read_view_close_timeout\316\000\001Q\200\001\224\246insert\315\002\023\304\030\222\261raft_wal_size_max\316\004\000\000\000\001\224\246insert\315\002\023\304\025\222\262raft_wal_count_max@\001\224\246insert\315\002\023\304(\222\275governor_auto_offline_timeout\313@>\000\000\000\000\000\000\001\224\246insert\315\002\023\304#\222\270governor_raft_op_timeout\313@\010\000\000\000\000\000\000\001\224\246insert\315\002\023\304&\222\273governor_common_rpc_timeout\313@\010\000\000\000\000\000\000\001\224\246insert\315\002\023\304&\222\273governor_plugin_rpc_timeout\313@$\000\000\000\000\000\000\001\224\246insert\315\002\023\304\030\222\263sql_vdbe_opcode_max\315\257\310\001\224\246insert\315\002\023\304\027\222\262sql_motion_row_max\315\023\210\001\224\246insert\315\002\023\304\031\222\266memtx_checkpoint_count\002\001\224\246insert\315\002\023\304\036\222\271memtx_checkpoint_interval\315\016\020\001\224\246insert\315\002\023\304\027\222\262iproto_net_msg_max\315\003\000\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.554 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 3 entries {term: 1 index: 4 context: "\222\251batch_dml\334\000\027\224\246insert\315\002\010\3049\226\000\245guest\000\222\243md5\331#md5084e0343a0486ff05530df6c705c8bb4\001\244user\001\224\246insert\315\002\t\304\024\226\001\000\245login\250universe\000\000\001\224\246insert\315\002\t\304\022\226\001\000\247execute\244role\002\000\001\224\246insert\315\002\010\304\025\226\001\245admin\000\222\243md5\240\001\244user\001\224\246insert\315\002\t\304\023\226\001\001\244read\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245write\250universe\000\000\001\224\246insert\315\002\t\304\026\226\001\001\247execute\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245login\250universe\000\000\001\224\246insert\315\002\t\304\025\226\001\001\246create\250universe\000\000\001\224\246insert\315\002\t\304\023\226\001\001\244drop\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245alter\250universe\000\000\001\224\246insert\315\002\010\304>\226 \254pico_service\000\222\251chap-sha1\274JHDAwG3uQv0WGLuZAFrcouydHhk=\001\244user\001\224\246insert\315\002\t\304\023\226\001 \244read\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245write\250universe\000\000\001\224\246insert\315\002\t\304\026\226\001 \247execute\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245login\250universe\000\000\001\224\246insert\315\002\t\304\025\226\001 \246create\250universe\000\000\001\224\246insert\315\002\t\304\023\226\001 \244drop\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245alter\250universe\000\000\001\224\246insert\315\002\t\304\022\226\001 \247execute\244role\003\000\001\224\246insert\315\002\010\304\021\226\002\246public\000\300\001\244role\001\224\246insert\315\002\010\304\020\226\037\245super\000\300\001\244role\001\224\246insert\315\002\010\304\026\226\003\253replication\000\300\001\244role\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.555 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 4 entries {term: 1 index: 5 context: "\226\243acl\253change_auth\000\222\243md5\331#md5084e0343a0486ff05530df6c705c8bb4\001\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.555 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 5 entries {term: 1 index: 6 context: "\222\251batch_dml\334\000,\224\246insert\315\002\000\305\001\370\231\315\002\000\253_pico_table\201\246Global\300\231\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\254distribution\252field_type\243map\253is_nullable\302\203\244name\246format\252field_type\245array\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\246engine\252field_type\246string\253is_nullable\302\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\000\303\245memtx\001\3316Stores metadata of all the cluster tables in picodata.\001\224\246insert\315\002\001\3046\230\315\002\000\000\256_pico_table_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3048\230\315\002\000\001\260_pico_table_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304?\230\315\002\000\002\264_pico_table_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001\212\231\315\002\001\253_pico_index\201\246Global\300\230\203\244name\250table_id\252field_type\250unsigned\253is_nullable\302\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244type\252field_type\246string\253is_nullable\302\203\244name\244opts\252field_type\245array\253is_nullable\302\203\244name\245parts\252field_type\245array\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304L\230\315\002\001\000\256_pico_index_id\244tree\221\201\246unique\303\222\225\250table_id\250unsigned\300\302\300\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3048\230\315\002\001\001\260_pico_index_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304\207\231\315\002\002\262_pico_peer_address\201\246Global\300\222\203\244name\247raft_id\252field_type\250unsigned\253is_nullable\302\203\244name\247address\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304G\230\315\002\002\000\272_pico_peer_address_raft_id\244tree\221\201\246unique\303\221\225\247raft_id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\002\010\231\315\002\003\256_pico_instance\201\246Global\300\232\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244uuid\252field_type\246string\253is_nullable\302\203\244name\247raft_id\252field_type\250unsigned\253is_nullable\302\203\244name\257replicaset_name\252field_type\246string\253is_nullable\302\203\244name\257replicaset_uuid\252field_type\246string\253is_nullable\302\203\244name\255current_state\252field_type\245array\253is_nullable\302\203\244name\254target_state\252field_type\245array\253is_nullable\302\203\244name\256failure_domain\252field_type\243map\253is_nullable\302\203\244name\244tier\252field_type\246string\253is_nullable\302\203\244name\260picodata_version\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304;\230\315\002\003\000\263_pico_instance_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304;\230\315\002\003\001\263_pico_instance_uuid\244tree\221\201\246unique\303\221\225\244uuid\246string\300\302\300\303\000\001\224\246insert\315\002\001\304C\230\315\002\003\002\266_pico_instance_raft_id\244tree\221\201\246unique\303\221\225\247raft_id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\304Q\230\315\002\003\003\276_pico_instance_replicaset_name\244tree\221\201\246unique\302\221\225\257replicaset_name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304x\231\315\002\004\256_pico_property\201\246Global\300\222\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3049\230\315\002\004\000\262_pico_property_key\244tree\221\201\246unique\303\221\225\243key\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\002P\231\315\002\005\260_pico_replicaset\201\246Global\300\233\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244uuid\252field_type\246string\253is_nullable\302\203\244name\263current_master_name\252field_type\246string\253is_nullable\302\203\244name\262target_master_name\252field_type\246string\253is_nullable\302\203\244name\244tier\252field_type\246string\253is_nullable\302\203\244name\246weight\252field_type\246double\253is_nullable\302\203\244name\255weight_origin\252field_type\246string\253is_nullable\302\203\244name\245state\252field_type\246string\253is_nullable\302\203\244name\266current_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\265target_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\260promotion_vclock\252field_type\243map\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304=\230\315\002\005\000\265_pico_replicaset_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304=\230\315\002\005\001\265_pico_replicaset_uuid\244tree\221\201\246unique\303\221\225\244uuid\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001-\231\315\002\010\252_pico_user\201\246Global\300\226\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\244auth\252field_type\245array\253is_nullable\303\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\203\244name\244type\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3045\230\315\002\010\000\255_pico_user_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3047\230\315\002\010\001\257_pico_user_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304>\230\315\002\010\002\263_pico_user_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001R\231\315\002\t\257_pico_privilege\201\246Global\300\226\203\244name\252grantor_id\252field_type\250unsigned\253is_nullable\302\203\244name\252grantee_id\252field_type\250unsigned\253is_nullable\302\203\244name\251privilege\252field_type\246string\253is_nullable\302\203\244name\253object_type\252field_type\246string\253is_nullable\302\203\244name\251object_id\252field_type\247integer\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304\211\230\315\002\t\000\267_pico_privilege_primary\244tree\221\201\246unique\303\224\225\252grantee_id\250unsigned\300\302\300\225\253object_type\246string\300\302\300\225\251object_id\247integer\300\302\300\225\251privilege\246string\300\302\300\303\000\001\224\246insert\315\002\001\304[\230\315\002\t\001\266_pico_privilege_object\244tree\221\201\246unique\302\222\225\253object_type\246string\300\302\300\225\251object_id\247integer\300\302\300\303\000\001\224\246insert\315\002\000\305\001y\231\315\002\013\252_pico_tier\201\246Global\300\226\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\262replication_factor\252field_type\250unsigned\253is_nullable\302\203\244name\250can_vote\252field_type\247boolean\253is_nullable\302\203\244name\275current_vshard_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\274target_vshard_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\263vshard_bootstrapped\252field_type\247boolean\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3047\230\315\002\013\000\257_pico_tier_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\002\023\231\315\002\014\255_pico_routine\201\246Global\300\233\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244kind\252field_type\246string\253is_nullable\302\203\244name\246params\252field_type\245array\253is_nullable\302\203\244name\247returns\252field_type\245array\253is_nullable\302\203\244name\250language\252field_type\246string\253is_nullable\302\203\244name\244body\252field_type\246string\253is_nullable\302\203\244name\250security\252field_type\246string\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3048\230\315\002\014\000\260_pico_routine_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\304:\230\315\002\014\001\262_pico_routine_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304A\230\315\002\014\002\266_pico_routine_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001;\231\315\002\016\254_pico_plugin\201\246Global\300\226\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\247enabled\252field_type\247boolean\253is_nullable\302\203\244name\250services\252field_type\245array\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\203\244name\256migration_list\252field_type\245array\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304L\230\315\002\016\000\261_pico_plugin_name\244tree\221\201\246unique\303\222\225\244name\246string\300\302\300\225\247version\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001\t\231\315\002\017\255_pico_service\201\246Global\300\225\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\245tiers\252field_type\245array\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304d\230\315\002\017\000\262_pico_service_name\244tree\221\201\246unique\303\223\225\253plugin_name\246string\300\302\300\225\244name\246string\300\302\300\225\247version\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001#\231\315\002\020\263_pico_service_route\201\246Global\300\225\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\256plugin_version\252field_type\246string\253is_nullable\302\203\244name\254service_name\252field_type\246string\253is_nullable\302\203\244name\255instance_name\252field_type\246string\253is_nullable\302\203\244name\246poison\252field_type\247boolean\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304\223\230\315\002\020\000\271_pico_service_routing_key\244tree\221\201\246unique\303\224\225\253plugin_name\246string\300\302\300\225\256plugin_version\246string\300\302\300\225\254service_name\246string\300\302\300\225\255instance_name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304\276\231\315\002\021\266_pico_plugin_migration\201\246Global\300\223\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\256migration_file\252field_type\246string\253is_nullable\302\203\244name\244hash\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304l\230\315\002\021\000\331\"_pico_plugin_migration_primary_key\244tree\221\201\246unique\303\222\225\253plugin_name\246string\300\302\300\225\256migration_file\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001\002\231\315\002\022\263_pico_plugin_config\201\246Global\300\225\203\244name\246plugin\252field_type\246string\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\246entity\252field_type\246string\253is_nullable\302\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\303\000\303\245memtx\001\240\001\224\246insert\315\002\001\304t\230\315\002\022\000\266_pico_plugin_config_pk\244tree\221\201\246unique\303\224\225\246plugin\246string\300\302\300\225\247version\246string\300\302\300\225\246entity\246string\300\302\300\225\243key\246string\300\302\300\303\000\001\224\246insert\315\002\000\304y\231\315\002\023\257_pico_db_config\201\246Global\300\222\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304:\230\315\002\023\000\263_pico_db_config_key\244tree\221\201\246unique\303\221\225\243key\246string\300\302\300\303\000\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.555 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 1 index: 6 entries {entry_type: EntryConfChange term: 1 index: 7 data: "\030\001"} entries {term: 2 index: 8} commit: 15, to: 2, from: 1, raft_id: 1
:3301 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 8 entries {term: 2 index: 9 context: "\222\251batch_dml\223\225\246update\315\002\003\304\r\221\253default_1_1\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\001\001\225\246update\315\002\013\304\t\221\247default\221\304!\223\241=\274target_vshard_config_version\001\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 9 entries {term: 2 index: 10 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\001\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 10 entries {term: 2 index: 11 context: "\226\243dml\246update\315\002\003\304\r\221\253default_1_1\221\304\032\223\241=\255current_state\222\246Online\001\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 11 entries {term: 2 index: 12 context: "\222\251batch_dml\222\224\247replace\315\002\002\304\021\222\002\256127.0.0.1:3302\001\224\247replace\315\002\003\304\230\232\253default_1_2\331$caf0b1e6-ae28-4955-9c34-35169d154b1a\002\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 12 entries {entry_type: EntryConfChangeV2 term: 2 index: 13 data: "\022\004\010\002\020\002"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 13 entries {term: 2 index: 14 context: "\222\251batch_dml\222\224\247replace\315\002\002\304\021\222\003\256127.0.0.1:3304\001\224\247replace\315\002\003\304\230\232\253default_1_3\331$35662ba5-d645-45bb-89b6-0a4553e26627\003\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/128/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 14 entries {entry_type: EntryConfChangeV2 term: 2 index: 15 data: "\022\004\010\002\020\003"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.556 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.556 [771631] main/114/raft_main_loop V> done sending messages, sent: 13, skipped: 0
:3304 | 2025-02-17 23:38:28.556 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.557 [771631] main/129/main I> initial data sent.
:3304 | 2025-02-17 23:38:28.557 [771631] main/129/main I> assigned id 3 to replica 35662ba5-d645-45bb-89b6-0a4553e26627
:3304 | 2025-02-17 23:38:28.557 [771631] main I> update replication_synchro_quorum = 2
:3304 | 2025-02-17 23:38:28.557 [771631] main/102/txn_limbo_on_parameters_change I> handling parameter changes affecting the limbo
:3304 | 2025-02-17 23:38:28.557 [771631] relay/127.0.0.1:40498/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3304 | 2025-02-17 23:38:28.557 [771631] main/129/main I> final data sent.
:3304 | 2025-02-17 23:38:28.570 [771631] main/141/main I> subscribed replica 35662ba5-d645-45bb-89b6-0a4553e26627 at fd 34, aka 127.0.0.1:3300, peer of 127.0.0.1:40498
:3304 | 2025-02-17 23:38:28.570 [771631] main/141/main I> remote vclock {1: 150} local vclock {0: 177, 1: 150}
:3304 | 2025-02-17 23:38:28.571 [771631] relay/127.0.0.1:40498/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3304 | 2025-02-17 23:38:28.729 [771631] main/142/.proc_update_instance V> got update instance request: Request { instance_name: InstanceName("default_1_2"), cluster_name: "cluster-0-0", current_state: None, target_state: Some(Online), failure_domain: Some(FailureDomain), dont_retry: false, picodata_version: Some("24.7.0-1217-ge3d68f09f") }
:3304 | 2025-02-17 23:38:28.730 [771631] main/142/.proc_update_instance update_instance.rs:278 E> joinee version is 24.7.0-1217-ge3d68f09f
:3304 | 2025-02-17 23:38:28.731 [771631] main/142/.proc_update_instance V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 15 entries {term: 2 index: 16 context: "\222\251batch_dml\222\225\246update\315\002\003\304\r\221\253default_1_2\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\002\001"} commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.731 [771631] main/142/.proc_update_instance V> waiting for applied index 16
:3304 | 2025-02-17 23:38:28.764 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgHeartbeat to: 2 commit: 15, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.764 [771631] main/114/raft_main_loop V> Sending from 1 to 3, msg: msg_type: MsgHeartbeat to: 3, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.764 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.764 [771631] main/114/raft_main_loop V> done sending messages, sent: 3, skipped: 0
:3304 | 2025-02-17 23:38:28.764 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3304 | 2025-02-17 23:38:28.765 [771631] main/114/raft_main_loop V> persisted index 16, raft_id: 1
:3304 | 2025-02-17 23:38:28.765 [771631] main/114/raft_main_loop V> committing index 16, index: 16, raft_id: 1
:3304 | 2025-02-17 23:38:28.766 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 16 commit: 16, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.766 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.766 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.766 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3304 | 2025-02-17 23:38:28.766 [771631] main/114/raft_main_loop V> commit index: 16
:3304 | 2025-02-17 23:38:28.768 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3304 | 2025-02-17 23:38:28.768 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Update(_pico_instance, ["default_1_2"], [["=","target_state",["Online",1]]]), Update(_pico_replicaset, ["default_1"], [["=","target_config_version",2]])), index: 16
:3304 | {"id":"1.0.19","initiator":"admin","instance_name":"default_1_2","message":"target state of instance `default_1_2` changed to Online(1)","new_state":"Online(1)","raft_id":"2","severity":"low","time":"2025-02-17T23:38:28.768+0300","title":"change_target_state"}
:3304 | 2025-02-17 23:38:28.770 [771631] main/115/governor_loop V> governor_loop_status = #8 'configure replication'
:3304 | 2025-02-17 23:38:28.770 [771631] main/115/governor_loop I> configuring replication
:3304 | 2025-02-17 23:38:28.770 [771631] main/115/governor_loop I> calling proc_replication, is_master: true, instance_name: default_1_1
:3304 | 2025-02-17 23:38:28.770 [771631] main/115/governor_loop I> calling proc_replication, is_master: false, instance_name: default_1_2
:3304 | 2025-02-17 23:38:28.771 [771631] main/143/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.771 [771631] main/140/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.771 [771631] main/139/.proc_replication I> connecting to 3 replicas
:3304 | 2025-02-17 23:38:28.772 [771631] main/139/.proc_replication C> failed to connect to 3 out of 3 replicas
:3304 | 2025-02-17 23:38:28.773 [771631] main/139/.proc_replication I> leaving orphan mode
:3304 | 2025-02-17 23:38:28.773 [771631] main/139/.proc_replication/box.load_cfg I> set 'replication' configuration option to ["pico_service@127.0.0.1:3300","pico_service@127.0.0.1:3302","pico_service@127.0.0.1:3304"]
:3304 | 2025-02-17 23:38:28.773 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.773 [771631] main/142/.proc_update_instance V> done waiting for applied index 16, current: 16
:3304 | 2025-02-17 23:38:28.773 [771631] main/114/raft_main_loop V> can't lock mutex at src/traft/node.rs:2570:45, already locked at unknown location
:3304 | 2025-02-17 23:38:28.773 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> remote master caf0b1e6-ae28-4955-9c34-35169d154b1a at 127.0.0.1:3302 running Tarantool 2.11.5
:3304 | 2025-02-17 23:38:28.773 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_1
:3304 | 2025-02-17 23:38:28.773 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_2
:3304 | 2025-02-17 23:38:28.773 [771631] main/115/governor_loop I> actualizing replicaset configuration version, replicaset_name: default_1
:3304 | 2025-02-17 23:38:28.773 [771631] main/115/governor_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 16 entries {term: 2 index: 17 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\002\001"} commit: 16, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.773 [771631] main/115/governor_loop V> waiting for applied index 17
:3304 | 2025-02-17 23:38:28.773 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.773 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.773 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3304 | 2025-02-17 23:38:28.774 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> remote master 35662ba5-d645-45bb-89b6-0a4553e26627 at 127.0.0.1:3304 running Tarantool 2.11.5
:3304 | 2025-02-17 23:38:28.774 [771631] relay/127.0.0.1:40480/101/main coio.c:353 E> SocketError: unexpected EOF when reading from socket, called on fd 26, aka 127.0.0.1:3300, peer of 127.0.0.1:40480: Broken pipe
:3304 | 2025-02-17 23:38:28.774 [771631] relay/127.0.0.1:40480/101/main I> exiting the relay loop
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> persisted index 17, raft_id: 1
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> committing index 17, index: 17, raft_id: 1
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 17 commit: 17, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> commit index: 17
:3304 | 2025-02-17 23:38:28.774 [771631] main/151/applier/pico_service@127.0.0.1:3300 I> remote master 77fdab61-0002-4cd4-a9a9-c92f78af29e4 at 127.0.0.1:3300 running Tarantool 2.11.5
:3304 | 2025-02-17 23:38:28.774 [771631] main/143/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3304 | 2025-02-17 23:38:28.774 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_replicaset, ["default_1"], [["=","current_config_version",2]]), index: 17
:3304 | 2025-02-17 23:38:28.775 [771631] main/110/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.775 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.775 [771631] main/115/governor_loop V> done waiting for applied index 17, current: 17
:3304 | 2025-02-17 23:38:28.775 [771631] main/115/governor_loop V> governor_loop_status = #9 'update instance state to online'
:3304 | 2025-02-17 23:38:28.775 [771631] main/115/governor_loop I> enable plugins on instance, instance_name: default_1_2
:3304 | 2025-02-17 23:38:28.775 [771631] main/110/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 14 entries {entry_type: EntryConfChangeV2 term: 2 index: 15 data: "\022\004\010\002\020\003"} commit: 17, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.775 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.775 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.775 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.776 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> authenticated
:3304 | 2025-02-17 23:38:28.776 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> subscribed
:3304 | 2025-02-17 23:38:28.776 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> remote vclock {1: 150} local vclock {0: 186, 1: 150}
:3304 | 2025-02-17 23:38:28.776 [771631] main/151/applier/pico_service@127.0.0.1:3300 I> leaving orphan mode
:3304 | 2025-02-17 23:38:28.776 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> RAFT: message {term: 1, state: follower} from 2
:3304 | 2025-02-17 23:38:28.776 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> authenticated
:3304 | 2025-02-17 23:38:28.776 [771631] main/115/governor_loop I> handling instance state change, current_state: Online, instance_name: default_1_2
:3304 | 2025-02-17 23:38:28.776 [771631] main/115/governor_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 17 entries {term: 2 index: 18 context: "\226\243dml\246update\315\002\003\304\r\221\253default_1_2\221\304\032\223\241=\255current_state\222\246Online\001\001"} commit: 17, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.776 [771631] main/115/governor_loop V> waiting for applied index 18
:3304 | 2025-02-17 23:38:28.776 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> subscribed
:3304 | 2025-02-17 23:38:28.776 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> remote vclock {1: 150} local vclock {0: 186, 1: 150}
:3304 | 2025-02-17 23:38:28.776 [771631] main/110/.proc_raft_interact V> received msgAppend rejection, index: 14, from: 3, reject_hint_term: 0, reject_hint_index: 0, raft_id: 1
:3304 | 2025-02-17 23:38:28.776 [771631] main/110/.proc_raft_interact V> decreased progress of 3, progress: Progress { matched: 0, next_idx: 1, state: Probe, paused: false, pending_snapshot: 0, pending_request_snapshot: 0, recent_active: true, ins: Inflights { start: 0, count: 0, buffer: [], cap: 256, incoming_cap: None }, commit_group_id: 0, committed_index: 0 }, raft_id: 1
:3304 | 2025-02-17 23:38:28.777 [771631] main/110/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 entries {term: 1 index: 1 context: "\222\251batch_dml\223\224\247replace\315\002\002\304\021\222\001\256127.0.0.1:3300\001\224\246insert\315\002\003\304\230\232\253default_1_1\331$77fdab61-0002-4cd4-a9a9-c92f78af29e4\001\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001\224\246insert\315\002\005\304l\233\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\253default_1_1\253default_1_1\247default\313\000\000\000\000\000\000\000\000\244auto\251not-ready\000\000\200\001"} commit: 17, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3304 | 2025-02-17 23:38:28.777 [771631] main/150/applier/pico_service@127.0.0.1:3302 I> leaving orphan mode
:3304 | 2025-02-17 23:38:28.777 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> RAFT: message {term: 1, state: follower} from 3
:3304 | 2025-02-17 23:38:28.777 [771631] main/149/applier/pico_service@127.0.0.1:3304 I> leaving orphan mode
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> persisted index 18, raft_id: 1
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> committing index 18, index: 18, raft_id: 1
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 18 commit: 18, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3304 | 2025-02-17 23:38:28.777 [771631] main/114/raft_main_loop V> commit index: 18
:3304 | 2025-02-17 23:38:28.778 [771631] main/110/main I> subscribed replica caf0b1e6-ae28-4955-9c34-35169d154b1a at fd 46, aka 127.0.0.1:3300, peer of 127.0.0.1:40546
:3304 | 2025-02-17 23:38:28.778 [771631] main/110/main I> remote vclock {1: 150} local vclock {0: 187, 1: 150}
:3304 | 2025-02-17 23:38:28.778 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3304 | 2025-02-17 23:38:28.778 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_instance, ["default_1_2"], [["=","current_state",["Online",1]]]), index: 18
:3304 | {"id":"1.0.20","initiator":"admin","instance_name":"default_1_2","message":"current state of instance `default_1_2` changed to Online(1)","new_state":"Online(1)","raft_id":"2","severity":"medium","time":"2025-02-17T23:38:28.777+0300","title":"change_current_state"}
:3304 | 2025-02-17 23:38:28.778 [771631] main/143/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.778 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.778 [771631] main/115/governor_loop V> done waiting for applied index 18, current: 18
:3304 | 2025-02-17 23:38:28.778 [771631] relay/127.0.0.1:40546/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3304 | 2025-02-17 23:38:28.778 [771631] main/115/governor_loop V> governor_loop_status = #10 'conf change'
:3304 | 2025-02-17 23:38:28.778 [771631] main/115/governor_loop I> proposing conf_change, cc: changes {node_id: 2}
:3304 | 2025-02-17 23:38:28.778 [771631] main/115/governor_loop V> can't lock mutex at src/traft/node.rs:444:18, already locked at unknown location
:3304 | 2025-02-17 23:38:28.778 [771631] main/115/governor_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 18 entries {entry_type: EntryConfChangeV2 term: 2 index: 19 data: "\022\002\020\002"} commit: 18, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.778 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3304 | 2025-02-17 23:38:28.779 [771631] main/143/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.779 [771631] main/140/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> persisted index 19, raft_id: 1
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> committing index 19, index: 19, raft_id: 1
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 19 commit: 19, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> done sending messages, sent: 1, skipped: 0
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting commit index'
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> commit index: 19
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1, 2} }, outgoing: Configuration { voters: {} } }, learners: {3}, learners_next: {}, auto_leave: false }, raft_id: 1
:3304 | 2025-02-17 23:38:28.779 [771631] main/115/governor_loop V> governor_loop_status = #11 'idle'
:3304 | 2025-02-17 23:38:28.779 [771631] main/115/governor_loop I> nothing to do, waiting for events to handle
:3304 | 2025-02-17 23:38:28.779 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.780 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 1 entries {term: 1 index: 2 context: "\222\251batch_dml\221\224\246insert\315\002\013\304\016\226\247default\003\303\000\000\302\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.781 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 2 entries {term: 1 index: 3 context: "\222\251batch_dml\334\000\031\224\246insert\315\002\004\304\030\222\265global_schema_version\000\001\224\246insert\315\002\004\304\026\222\263next_schema_version\001\001\224\246insert\315\002\004\304\037\222\266system_catalog_version\24625.1.0\001\224\246insert\315\002\004\304(\222\257cluster_version\26624.7.0-1217-ge3d68f09f\001\224\246insert\315\002\023\304\033\222\270auth_password_length_min\010\001\224\246insert\315\002\023\304\"\222\277auth_password_enforce_uppercase\303\001\224\246insert\315\002\023\304\"\222\277auth_password_enforce_lowercase\303\001\224\246insert\315\002\023\304\037\222\274auth_password_enforce_digits\303\001\224\246insert\315\002\023\304&\222\331\"auth_password_enforce_specialchars\302\001\224\246insert\315\002\023\304\031\222\266auth_login_attempt_max\004\001\224\246insert\315\002\023\304\025\222\260pg_statement_max\315\004\000\001\224\246insert\315\002\023\304\022\222\255pg_portal_max\315\004\000\001\224\246insert\315\002\023\304#\222\274raft_snapshot_chunk_size_max\316\001\000\000\000\001\224\246insert\315\002\023\304-\222\331%raft_snapshot_read_view_close_timeout\316\000\001Q\200\001\224\246insert\315\002\023\304\030\222\261raft_wal_size_max\316\004\000\000\000\001\224\246insert\315\002\023\304\025\222\262raft_wal_count_max@\001\224\246insert\315\002\023\304(\222\275governor_auto_offline_timeout\313@>\000\000\000\000\000\000\001\224\246insert\315\002\023\304#\222\270governor_raft_op_timeout\313@\010\000\000\000\000\000\000\001\224\246insert\315\002\023\304&\222\273governor_common_rpc_timeout\313@\010\000\000\000\000\000\000\001\224\246insert\315\002\023\304&\222\273governor_plugin_rpc_timeout\313@$\000\000\000\000\000\000\001\224\246insert\315\002\023\304\030\222\263sql_vdbe_opcode_max\315\257\310\001\224\246insert\315\002\023\304\027\222\262sql_motion_row_max\315\023\210\001\224\246insert\315\002\023\304\031\222\266memtx_checkpoint_count\002\001\224\246insert\315\002\023\304\036\222\271memtx_checkpoint_interval\315\016\020\001\224\246insert\315\002\023\304\027\222\262iproto_net_msg_max\315\003\000\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.782 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 3 entries {term: 1 index: 4 context: "\222\251batch_dml\334\000\027\224\246insert\315\002\010\3049\226\000\245guest\000\222\243md5\331#md5084e0343a0486ff05530df6c705c8bb4\001\244user\001\224\246insert\315\002\t\304\024\226\001\000\245login\250universe\000\000\001\224\246insert\315\002\t\304\022\226\001\000\247execute\244role\002\000\001\224\246insert\315\002\010\304\025\226\001\245admin\000\222\243md5\240\001\244user\001\224\246insert\315\002\t\304\023\226\001\001\244read\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245write\250universe\000\000\001\224\246insert\315\002\t\304\026\226\001\001\247execute\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245login\250universe\000\000\001\224\246insert\315\002\t\304\025\226\001\001\246create\250universe\000\000\001\224\246insert\315\002\t\304\023\226\001\001\244drop\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001\001\245alter\250universe\000\000\001\224\246insert\315\002\010\304>\226 \254pico_service\000\222\251chap-sha1\274JHDAwG3uQv0WGLuZAFrcouydHhk=\001\244user\001\224\246insert\315\002\t\304\023\226\001 \244read\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245write\250universe\000\000\001\224\246insert\315\002\t\304\026\226\001 \247execute\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245login\250universe\000\000\001\224\246insert\315\002\t\304\025\226\001 \246create\250universe\000\000\001\224\246insert\315\002\t\304\023\226\001 \244drop\250universe\000\000\001\224\246insert\315\002\t\304\024\226\001 \245alter\250universe\000\000\001\224\246insert\315\002\t\304\022\226\001 \247execute\244role\003\000\001\224\246insert\315\002\010\304\021\226\002\246public\000\300\001\244role\001\224\246insert\315\002\010\304\020\226\037\245super\000\300\001\244role\001\224\246insert\315\002\010\304\026\226\003\253replication\000\300\001\244role\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.782 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 4 entries {term: 1 index: 5 context: "\226\243acl\253change_auth\000\222\243md5\331#md5084e0343a0486ff05530df6c705c8bb4\001\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.783 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 5 entries {term: 1 index: 6 context: "\222\251batch_dml\334\000,\224\246insert\315\002\000\305\001\370\231\315\002\000\253_pico_table\201\246Global\300\231\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\254distribution\252field_type\243map\253is_nullable\302\203\244name\246format\252field_type\245array\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\246engine\252field_type\246string\253is_nullable\302\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\000\303\245memtx\001\3316Stores metadata of all the cluster tables in picodata.\001\224\246insert\315\002\001\3046\230\315\002\000\000\256_pico_table_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3048\230\315\002\000\001\260_pico_table_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304?\230\315\002\000\002\264_pico_table_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001\212\231\315\002\001\253_pico_index\201\246Global\300\230\203\244name\250table_id\252field_type\250unsigned\253is_nullable\302\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244type\252field_type\246string\253is_nullable\302\203\244name\244opts\252field_type\245array\253is_nullable\302\203\244name\245parts\252field_type\245array\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304L\230\315\002\001\000\256_pico_index_id\244tree\221\201\246unique\303\222\225\250table_id\250unsigned\300\302\300\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3048\230\315\002\001\001\260_pico_index_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304\207\231\315\002\002\262_pico_peer_address\201\246Global\300\222\203\244name\247raft_id\252field_type\250unsigned\253is_nullable\302\203\244name\247address\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304G\230\315\002\002\000\272_pico_peer_address_raft_id\244tree\221\201\246unique\303\221\225\247raft_id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\002\010\231\315\002\003\256_pico_instance\201\246Global\300\232\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244uuid\252field_type\246string\253is_nullable\302\203\244name\247raft_id\252field_type\250unsigned\253is_nullable\302\203\244name\257replicaset_name\252field_type\246string\253is_nullable\302\203\244name\257replicaset_uuid\252field_type\246string\253is_nullable\302\203\244name\255current_state\252field_type\245array\253is_nullable\302\203\244name\254target_state\252field_type\245array\253is_nullable\302\203\244name\256failure_domain\252field_type\243map\253is_nullable\302\203\244name\244tier\252field_type\246string\253is_nullable\302\203\244name\260picodata_version\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304;\230\315\002\003\000\263_pico_instance_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304;\230\315\002\003\001\263_pico_instance_uuid\244tree\221\201\246unique\303\221\225\244uuid\246string\300\302\300\303\000\001\224\246insert\315\002\001\304C\230\315\002\003\002\266_pico_instance_raft_id\244tree\221\201\246unique\303\221\225\247raft_id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\304Q\230\315\002\003\003\276_pico_instance_replicaset_name\244tree\221\201\246unique\302\221\225\257replicaset_name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304x\231\315\002\004\256_pico_property\201\246Global\300\222\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3049\230\315\002\004\000\262_pico_property_key\244tree\221\201\246unique\303\221\225\243key\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\002P\231\315\002\005\260_pico_replicaset\201\246Global\300\233\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244uuid\252field_type\246string\253is_nullable\302\203\244name\263current_master_name\252field_type\246string\253is_nullable\302\203\244name\262target_master_name\252field_type\246string\253is_nullable\302\203\244name\244tier\252field_type\246string\253is_nullable\302\203\244name\246weight\252field_type\246double\253is_nullable\302\203\244name\255weight_origin\252field_type\246string\253is_nullable\302\203\244name\245state\252field_type\246string\253is_nullable\302\203\244name\266current_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\265target_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\260promotion_vclock\252field_type\243map\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304=\230\315\002\005\000\265_pico_replicaset_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304=\230\315\002\005\001\265_pico_replicaset_uuid\244tree\221\201\246unique\303\221\225\244uuid\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001-\231\315\002\010\252_pico_user\201\246Global\300\226\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\244auth\252field_type\245array\253is_nullable\303\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\203\244name\244type\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3045\230\315\002\010\000\255_pico_user_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\3047\230\315\002\010\001\257_pico_user_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304>\230\315\002\010\002\263_pico_user_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001R\231\315\002\t\257_pico_privilege\201\246Global\300\226\203\244name\252grantor_id\252field_type\250unsigned\253is_nullable\302\203\244name\252grantee_id\252field_type\250unsigned\253is_nullable\302\203\244name\251privilege\252field_type\246string\253is_nullable\302\203\244name\253object_type\252field_type\246string\253is_nullable\302\203\244name\251object_id\252field_type\247integer\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304\211\230\315\002\t\000\267_pico_privilege_primary\244tree\221\201\246unique\303\224\225\252grantee_id\250unsigned\300\302\300\225\253object_type\246string\300\302\300\225\251object_id\247integer\300\302\300\225\251privilege\246string\300\302\300\303\000\001\224\246insert\315\002\001\304[\230\315\002\t\001\266_pico_privilege_object\244tree\221\201\246unique\302\222\225\253object_type\246string\300\302\300\225\251object_id\247integer\300\302\300\303\000\001\224\246insert\315\002\000\305\001y\231\315\002\013\252_pico_tier\201\246Global\300\226\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\262replication_factor\252field_type\250unsigned\253is_nullable\302\203\244name\250can_vote\252field_type\247boolean\253is_nullable\302\203\244name\275current_vshard_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\274target_vshard_config_version\252field_type\250unsigned\253is_nullable\302\203\244name\263vshard_bootstrapped\252field_type\247boolean\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3047\230\315\002\013\000\257_pico_tier_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\002\023\231\315\002\014\255_pico_routine\201\246Global\300\233\203\244name\242id\252field_type\250unsigned\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\244kind\252field_type\246string\253is_nullable\302\203\244name\246params\252field_type\245array\253is_nullable\302\203\244name\247returns\252field_type\245array\253is_nullable\302\203\244name\250language\252field_type\246string\253is_nullable\302\203\244name\244body\252field_type\246string\253is_nullable\302\203\244name\250security\252field_type\246string\253is_nullable\302\203\244name\250operable\252field_type\247boolean\253is_nullable\302\203\244name\256schema_version\252field_type\250unsigned\253is_nullable\302\203\244name\245owner\252field_type\250unsigned\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\3048\230\315\002\014\000\260_pico_routine_id\244tree\221\201\246unique\303\221\225\242id\250unsigned\300\302\300\303\000\001\224\246insert\315\002\001\304:\230\315\002\014\001\262_pico_routine_name\244tree\221\201\246unique\303\221\225\244name\246string\300\302\300\303\000\001\224\246insert\315\002\001\304A\230\315\002\014\002\266_pico_routine_owner_id\244tree\221\201\246unique\302\221\225\245owner\250unsigned\300\302\300\303\000\001\224\246insert\315\002\000\305\001;\231\315\002\016\254_pico_plugin\201\246Global\300\226\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\247enabled\252field_type\247boolean\253is_nullable\302\203\244name\250services\252field_type\245array\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\203\244name\256migration_list\252field_type\245array\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304L\230\315\002\016\000\261_pico_plugin_name\244tree\221\201\246unique\303\222\225\244name\246string\300\302\300\225\247version\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001\t\231\315\002\017\255_pico_service\201\246Global\300\225\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\244name\252field_type\246string\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\245tiers\252field_type\245array\253is_nullable\302\203\244name\253description\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304d\230\315\002\017\000\262_pico_service_name\244tree\221\201\246unique\303\223\225\253plugin_name\246string\300\302\300\225\244name\246string\300\302\300\225\247version\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001#\231\315\002\020\263_pico_service_route\201\246Global\300\225\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\256plugin_version\252field_type\246string\253is_nullable\302\203\244name\254service_name\252field_type\246string\253is_nullable\302\203\244name\255instance_name\252field_type\246string\253is_nullable\302\203\244name\246poison\252field_type\247boolean\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304\223\230\315\002\020\000\271_pico_service_routing_key\244tree\221\201\246unique\303\224\225\253plugin_name\246string\300\302\300\225\256plugin_version\246string\300\302\300\225\254service_name\246string\300\302\300\225\255instance_name\246string\300\302\300\303\000\001\224\246insert\315\002\000\304\276\231\315\002\021\266_pico_plugin_migration\201\246Global\300\223\203\244name\253plugin_name\252field_type\246string\253is_nullable\302\203\244name\256migration_file\252field_type\246string\253is_nullable\302\203\244name\244hash\252field_type\246string\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304l\230\315\002\021\000\331\"_pico_plugin_migration_primary_key\244tree\221\201\246unique\303\222\225\253plugin_name\246string\300\302\300\225\256migration_file\246string\300\302\300\303\000\001\224\246insert\315\002\000\305\001\002\231\315\002\022\263_pico_plugin_config\201\246Global\300\225\203\244name\246plugin\252field_type\246string\253is_nullable\302\203\244name\247version\252field_type\246string\253is_nullable\302\203\244name\246entity\252field_type\246string\253is_nullable\302\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\303\000\303\245memtx\001\240\001\224\246insert\315\002\001\304t\230\315\002\022\000\266_pico_plugin_config_pk\244tree\221\201\246unique\303\224\225\246plugin\246string\300\302\300\225\247version\246string\300\302\300\225\246entity\246string\300\302\300\225\243key\246string\300\302\300\303\000\001\224\246insert\315\002\000\304y\231\315\002\023\257_pico_db_config\201\246Global\300\222\203\244name\243key\252field_type\246string\253is_nullable\302\203\244name\245value\252field_type\243any\253is_nullable\302\000\303\245memtx\001\240\001\224\246insert\315\002\001\304:\230\315\002\023\000\263_pico_db_config_key\244tree\221\201\246unique\303\221\225\243key\246string\300\302\300\303\000\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.783 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 1 index: 6 entries {entry_type: EntryConfChange term: 1 index: 7 data: "\030\001"} entries {term: 2 index: 8} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.783 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 8 entries {term: 2 index: 9 context: "\222\251batch_dml\223\225\246update\315\002\003\304\r\221\253default_1_1\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\001\001\225\246update\315\002\013\304\t\221\247default\221\304!\223\241=\274target_vshard_config_version\001\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.783 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 9 entries {term: 2 index: 10 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\001\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 10 entries {term: 2 index: 11 context: "\226\243dml\246update\315\002\003\304\r\221\253default_1_1\221\304\032\223\241=\255current_state\222\246Online\001\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 11 entries {term: 2 index: 12 context: "\222\251batch_dml\222\224\247replace\315\002\002\304\021\222\002\256127.0.0.1:3302\001\224\247replace\315\002\003\304\230\232\253default_1_2\331$caf0b1e6-ae28-4955-9c34-35169d154b1a\002\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 12 entries {entry_type: EntryConfChangeV2 term: 2 index: 13 data: "\022\004\010\002\020\002"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 13 entries {term: 2 index: 14 context: "\222\251batch_dml\222\224\247replace\315\002\002\304\021\222\003\256127.0.0.1:3304\001\224\247replace\315\002\003\304\230\232\253default_1_3\331$35662ba5-d645-45bb-89b6-0a4553e26627\003\251default_1\331$b33d17b0-46b7-43a4-9611-98bb29099f4d\222\247Offline\000\222\247Offline\000\200\247default\26624.7.0-1217-ge3d68f09f\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 14 entries {entry_type: EntryConfChangeV2 term: 2 index: 15 data: "\022\004\010\002\020\003"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.784 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 15 entries {term: 2 index: 16 context: "\222\251batch_dml\222\225\246update\315\002\003\304\r\221\253default_1_2\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\002\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.785 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 16 entries {term: 2 index: 17 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\002\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.785 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 17 entries {term: 2 index: 18 context: "\226\243dml\246update\315\002\003\304\r\221\253default_1_2\221\304\032\223\241=\255current_state\222\246Online\001\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.785 [771631] main/143/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 18 entries {entry_type: EntryConfChangeV2 term: 2 index: 19 data: "\022\002\020\002"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.785 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3304 | 2025-02-17 23:38:28.785 [771631] main/114/raft_main_loop V> done sending messages, sent: 17, skipped: 0
:3304 | 2025-02-17 23:38:28.785 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3304 | 2025-02-17 23:38:28.837 [771631] main/139/.proc_update_instance V> got update instance request: Request { instance_name: InstanceName("default_1_3"), cluster_name: "cluster-0-0", current_state: None, target_state: Some(Online), failure_domain: Some(FailureDomain), dont_retry: false, picodata_version: Some("24.7.0-1217-ge3d68f09f") }
:3304 | 2025-02-17 23:38:28.837 [771631] main/139/.proc_update_instance update_instance.rs:278 E> joinee version is 24.7.0-1217-ge3d68f09f
:3304 | 2025-02-17 23:38:28.838 [771631] main/139/.proc_update_instance V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 19 entries {term: 2 index: 20 context: "\222\251batch_dml\222\225\246update\315\002\003\304\r\221\253default_1_3\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\003\001"} commit: 19, to: 2, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.838 [771631] main/139/.proc_update_instance V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 19 entries {term: 2 index: 20 context: "\222\251batch_dml\222\225\246update\315\002\003\304\r\221\253default_1_3\221\304\031\223\241=\254target_state\222\246Online\001\001\225\246update\315\002\005\304\013\221\251default_1\221\304\032\223\241=\265target_config_version\003\001"} commit: 19, to: 3, from: 1, raft_id: 1
:3304 | 2025-02-17 23:38:28.838 [771631] main/139/.proc_update_instance V> waiting for applied index 20
:3300 | 2025-02-17 23:38:28.893 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:28.893 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3300 | 2025-02-17 23:38:28.893 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:28.894 [771631] main/114/raft_main_loop V> persisted index 20, raft_id: 1
:3300 | 2025-02-17 23:38:28.894 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:28.896 [771631] main/138/.proc_raft_interact V> committing index 20, index: 20, raft_id: 1
:3300 | 2025-02-17 23:38:28.896 [771631] main/138/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 20 commit: 20, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:28.896 [771631] main/138/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 20 commit: 20, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:28.896 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:28.896 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3300 | 2025-02-17 23:38:28.896 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:28.896 [771631] main/114/raft_main_loop V> hard state: term: 2 vote: 1 commit: 20
:3300 | 2025-02-17 23:38:28.897 [771631] main/138/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:28.897 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:28.897 [771631] main/114/raft_main_loop V> applying entry: BatchDml(Update(_pico_instance, ["default_1_3"], [["=","target_state",["Online",1]]]), Update(_pico_replicaset, ["default_1"], [["=","target_config_version",3]])), index: 20
:3300 | {"id":"1.0.21","initiator":"admin","instance_name":"default_1_3","message":"target state of instance `default_1_3` changed to Online(1)","new_state":"Online(1)","raft_id":"3","severity":"low","time":"2025-02-17T23:38:28.897+0300","title":"change_target_state"}
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop V> governor_loop_status = #12 'configure replication'
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> configuring replication
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> calling proc_replication, is_master: true, instance_name: default_1_1
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> calling proc_replication, is_master: false, instance_name: default_1_2
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> calling proc_replication, is_master: false, instance_name: default_1_3
:3300 | 2025-02-17 23:38:28.898 [771631] main/137/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:28.898 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:28.898 [771631] main/139/.proc_update_instance V> done waiting for applied index 20, current: 20
:3300 | 2025-02-17 23:38:28.898 [771631] main/114/raft_main_loop V> can't lock mutex at src/traft/node.rs:2570:45, already locked at unknown location
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_1
:3300 | 2025-02-17 23:38:28.898 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_2
:3300 | 2025-02-17 23:38:28.899 [771631] relay/127.0.0.1:40498/101/main coio.c:353 E> SocketError: unexpected EOF when reading from socket, called on fd 34, aka 127.0.0.1:3300, peer of 127.0.0.1:40498: Broken pipe
:3300 | 2025-02-17 23:38:28.899 [771631] relay/127.0.0.1:40498/101/main I> exiting the relay loop
:3300 | 2025-02-17 23:38:28.899 [771631] main/115/governor_loop I> configured replication with instance, instance_name: default_1_3
:3300 | 2025-02-17 23:38:28.899 [771631] main/115/governor_loop I> actualizing replicaset configuration version, replicaset_name: default_1
:3300 | 2025-02-17 23:38:28.900 [771631] main/115/governor_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 20 entries {term: 2 index: 21 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\003\001"} commit: 20, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:28.900 [771631] main/115/governor_loop V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 20 entries {term: 2 index: 21 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\221\304\033\223\241=\266current_config_version\003\001"} commit: 20, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:28.900 [771631] main/115/governor_loop V> waiting for applied index 21
:3300 | 2025-02-17 23:38:28.900 [771631] main/141/main I> subscribed replica 35662ba5-d645-45bb-89b6-0a4553e26627 at fd 32, aka 127.0.0.1:3300, peer of 127.0.0.1:40584
:3300 | 2025-02-17 23:38:28.900 [771631] main/141/main I> remote vclock {1: 150} local vclock {0: 205, 1: 150}
:3300 | 2025-02-17 23:38:28.900 [771631] relay/127.0.0.1:40584/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3300 | 2025-02-17 23:38:28.999 [771631] main/114/raft_main_loop V> Sending from 1 to 2, msg: msg_type: MsgHeartbeat to: 2 commit: 20, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.000 [771631] main/114/raft_main_loop V> Sending from 1 to 3, msg: msg_type: MsgHeartbeat to: 3 commit: 20, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.000 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:29.000 [771631] main/114/raft_main_loop V> done sending messages, sent: 4, skipped: 0
:3300 | 2025-02-17 23:38:29.000 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:29.001 [771631] main/114/raft_main_loop V> persisted index 21, raft_id: 1
:3300 | 2025-02-17 23:38:29.001 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:29.003 [771631] main/137/.proc_raft_interact V> committing index 21, index: 21, raft_id: 1
:3300 | 2025-02-17 23:38:29.003 [771631] main/137/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 21 commit: 21, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.003 [771631] main/137/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 21 commit: 21, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.003 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:29.004 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3300 | 2025-02-17 23:38:29.004 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:29.004 [771631] main/114/raft_main_loop V> hard state: term: 2 vote: 1 commit: 21
:3300 | 2025-02-17 23:38:29.004 [771631] main/138/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:29.004 [771631] main/137/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:29.004 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:29.004 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_replicaset, ["default_1"], [["=","current_config_version",3]]), index: 21
:3300 | 2025-02-17 23:38:29.005 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:29.005 [771631] main/115/governor_loop V> done waiting for applied index 21, current: 21
:3300 | 2025-02-17 23:38:29.006 [771631] main/115/governor_loop V> governor_loop_status = #13 'update replicaset state'
:3300 | 2025-02-17 23:38:29.006 [771631] main/115/governor_loop I> proposing replicaset state change
:3300 | 2025-02-17 23:38:29.006 [771631] main/115/governor_loop V> can't lock mutex at src/traft/node.rs:310:24, already locked at unknown location
:3300 | 2025-02-17 23:38:29.006 [771631] main/114/raft_main_loop V> can't lock mutex at src/traft/node.rs:2570:45, already locked at unknown location
:3300 | 2025-02-17 23:38:29.007 [771631] main/115/governor_loop V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 21 entries {term: 2 index: 22 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\222\304\023\223\241=\246weight\313?\360\000\000\000\000\000\000\304\017\223\241=\245state\245ready\001"} commit: 21, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.007 [771631] main/115/governor_loop V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 21 entries {term: 2 index: 22 context: "\226\243dml\246update\315\002\005\304\013\221\251default_1\222\304\023\223\241=\246weight\313?\360\000\000\000\000\000\000\304\017\223\241=\245state\245ready\001"} commit: 21, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.007 [771631] main/115/governor_loop V> waiting for applied index 22
:3300 | 2025-02-17 23:38:29.007 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:29.007 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3300 | 2025-02-17 23:38:29.007 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:29.008 [771631] main/137/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:29.008 [771631] main/138/.proc_raft_interact V> can't lock mutex at src/traft/node.rs:419:14, already locked at src/traft/node.rs:2570:45
:3300 | 2025-02-17 23:38:29.008 [771631] main/114/raft_main_loop V> persisted index 22, raft_id: 1
:3300 | 2025-02-17 23:38:29.008 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:29.010 [771631] main/138/.proc_raft_interact V> committing index 22, index: 22, raft_id: 1
:3300 | 2025-02-17 23:38:29.010 [771631] main/138/.proc_raft_interact V> Sending from 1 to 2, msg: msg_type: MsgAppend to: 2 log_term: 2 index: 22 commit: 22, to: 2, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.010 [771631] main/138/.proc_raft_interact V> Sending from 1 to 3, msg: msg_type: MsgAppend to: 3 log_term: 2 index: 22 commit: 22, to: 3, from: 1, raft_id: 1
:3300 | 2025-02-17 23:38:29.010 [771631] main/114/raft_main_loop V> main_loop_status = 'sending raft messages'
:3300 | 2025-02-17 23:38:29.010 [771631] main/114/raft_main_loop V> done sending messages, sent: 2, skipped: 0
:3300 | 2025-02-17 23:38:29.010 [771631] main/114/raft_main_loop V> main_loop_status = 'persisting hard state, entries and/or snapshot'
:3300 | 2025-02-17 23:38:29.010 [771631] main/114/raft_main_loop V> hard state: term: 2 vote: 1 commit: 22
:3300 | 2025-02-17 23:38:29.011 [771631] main/114/raft_main_loop V> main_loop_status = 'handling committed entries'
:3300 | 2025-02-17 23:38:29.011 [771631] main/114/raft_main_loop V> applying entry: Update(_pico_replicaset, ["default_1"], [["=","weight",1.0], ["=","state","ready"]]), index: 22
:3300 | 2025-02-17 23:38:29.011 [771631] main/114/raft_main_loop V> main_loop_status = 'idle'
:3300 | 2025-02-17 23:38:29.011 [771631] main/115/governor_loop V> done waiting for applied index 22, current: 22
:3300 | 2025-02-17 23:38:29.012 [771631] main/115/governor_loop V> governor_loop_status = #14 'update current sharding configuration'
:3300 | 2025-02-17 23:38:29.012 [771631] main/115/governor_loop I> applying vshard config changes, tier: default
:3300 | 2025-02-17 23:38:29.012 [771631] main/115/governor_loop I> calling proc_sharding, instance_name: default_1_1
:3300 | 2025-02-17 23:38:29.012 [771631] main/115/governor_loop I> calling proc_sharding, instance_name: default_1_2
:3300 | 2025-02-17 23:38:29.012 [771631] main/115/governor_loop I> calling proc_sharding, instance_name: default_1_3
:3300 | 2025-02-17 23:38:29.013 [771631] main/137/.proc_sharding V> waiting for applied index 22
:3300 | 2025-02-17 23:38:29.013 [771631] main/137/.proc_sharding V> done waiting for applied index 22, current: 22
:3300 | 2025-02-17 23:38:29.016 [771631] main/137/.proc_sharding/vshard.storage I> Starting configuration of replica 77fdab61-0002-4cd4-a9a9-c92f78af29e4
:3300 | 2025-02-17 23:38:29.016 [771631] main/137/.proc_sharding/vshard.storage I> I am master
:3300 | 2025-02-17 23:38:29.016 [771631] main/137/.proc_sharding/vshard.storage I> Taking on replicaset master role...
:3300 | 2025-02-17 23:38:29.016 [771631] main/137/.proc_sharding I> connecting to 3 replicas
:3300 | 2025-02-17 23:38:29.016 [771631] main/137/.proc_sharding C> failed to connect to 3 out of 3 replicas
:3300 | 2025-02-17 23:38:29.016 [771631] relay/127.0.0.1:40584/101/main coio.c:353 E> SocketError: unexpected EOF when reading from socket, called on fd 32, aka 127.0.0.1:3300, peer of 127.0.0.1:40584: Broken pipe
:3300 | 2025-02-17 23:38:29.017 [771631] relay/127.0.0.1:40584/101/main I> exiting the relay loop
:3300 | 2025-02-17 23:38:29.017 [771631] main/137/.proc_sharding I> leaving orphan mode
:3300 | 2025-02-17 23:38:29.017 [771631] main/137/.proc_sharding/box.load_cfg I> set 'replication' configuration option to ["pico_service@127.0.0.1:3304","pico_service@127.0.0.1:3302","pico_service@127.0.0.1:3300"]
:3300 | 2025-02-17 23:38:29.017 [771631] main/137/.proc_sharding/vshard.storage I> Box has been configured
:3300 | 2025-02-17 23:38:29.017 [771631] main/137/.proc_sharding/vshard.storage I> Initializing schema {0.1.15.0}
:3300 | 2025-02-17 23:38:29.017 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> remote master caf0b1e6-ae28-4955-9c34-35169d154b1a at 127.0.0.1:3302 running Tarantool 2.11.5
:3300 | 2025-02-17 23:38:29.017 [771631] main/159/applier/pico_service@127.0.0.1:3300 I> remote master 77fdab61-0002-4cd4-a9a9-c92f78af29e4 at 127.0.0.1:3300 running Tarantool 2.11.5
:3300 | 2025-02-17 23:38:29.017 [771631] relay/127.0.0.1:40546/101/main coio.c:353 E> SocketError: unexpected EOF when reading from socket, called on fd 46, aka 127.0.0.1:3300, peer of 127.0.0.1:40546: Broken pipe
:3300 | 2025-02-17 23:38:29.017 [771631] relay/127.0.0.1:40546/101/main I> exiting the relay loop
:3300 | 2025-02-17 23:38:29.018 [771631] main/159/applier/pico_service@127.0.0.1:3300 I> leaving orphan mode
:3300 | 2025-02-17 23:38:29.019 [771631] main/137/.proc_sharding/vshard.storage I> Upgrade vshard schema to {0.1.16.0}
:3300 | 2025-02-17 23:38:29.019 [771631] main/137/.proc_sharding/vshard.storage I> Insert 'vshard_version' into _schema
:3300 | 2025-02-17 23:38:29.019 [771631] main/137/.proc_sharding/vshard.storage I> Create function vshard.storage._call()
:3300 | 2025-02-17 23:38:29.019 [771631] main/137/.proc_sharding/vshard.storage I> Successful vshard schema upgrade to {0.1.16.0}
:3300 | 2025-02-17 23:38:29.019 [771631] main/137/.proc_sharding/vshard.storage I> Upgrade vshard.storage.bucket_recv to accept args as msgpack
:3300 | 2025-02-17 23:38:29.020 [771631] main/163/lua/vshard.util I> gc_bucket_f has been started
:3300 | 2025-02-17 23:38:29.020 [771631] main/163/lua/box.schema schema.lua:1832 W> box.internal.schema_version will be removed, please use box.info.schema_version instead
:3300 | 2025-02-17 23:38:29.020 [771631] main/164/lua/vshard.util I> recovery_f has been started
:3300 | 2025-02-17 23:38:29.020 [771631] main/137/.proc_sharding/vshard.storage I> Took on replicaset master role
:3300 | 2025-02-17 23:38:29.020 [771631] main/165/lua/vshard.util I> rebalancer_f has been started
:3300 | 2025-02-17 23:38:29.023 [771631] main/137/.proc_sharding/vshard.router I> Starting router configuration
:3300 | 2025-02-17 23:38:29.023 [771631] main/137/.proc_sharding/vshard.router I> Calling box.cfg()...
:3300 | 2025-02-17 23:38:29.023 [771631] main/137/.proc_sharding/vshard.router I> {"listen":"127.0.0.1:3300"}
:3300 | 2025-02-17 23:38:29.023 [771631] main/137/.proc_sharding/vshard.router I> Box has been configured
:3300 | 2025-02-17 23:38:29.024 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> remote master 35662ba5-d645-45bb-89b6-0a4553e26627 at 127.0.0.1:3304 running Tarantool 2.11.5
:3300 | 2025-02-17 23:38:29.024 [771631] main/115/governor_loop mod.rs:906 W> failed calling proc_sharding: server responded with error: NoSuchSpace: no such space "_bucket", instance_name: default_1_2
:3300 | 2025-02-17 23:38:29.024 [771631] main/115/governor_loop mod.rs:895 W> failed applying vshard config changes: server responded with error: NoSuchSpace: no such space "_bucket", tier: default
:3300 | 2025-02-17 23:38:29.024 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> authenticated
:3300 | 2025-02-17 23:38:29.025 [771631] main/167/127.0.0.1:3304 (net.box)/vshard.replicaset I> connected to 127.0.0.1:3304
:3300 | 2025-02-17 23:38:29.025 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> subscribed
:3300 | 2025-02-17 23:38:29.025 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> remote vclock {1: 150} local vclock {0: 217, 1: 190}
:3300 | 2025-02-17 23:38:29.026 [771631] main/169/127.0.0.1:3300 (net.box)/vshard.replicaset I> connected to 127.0.0.1:3300
:3300 | 2025-02-17 23:38:29.026 [771631] main/166/127.0.0.1:3300 (net.box)/vshard.replicaset I> connected to 127.0.0.1:3300
:3300 | 2025-02-17 23:38:29.026 [771631] main/173/lua/vshard.util I> failover_f has been started
:3300 | 2025-02-17 23:38:29.026 [771631] main/173/lua/vshard.router I> New replica default_1_1(pico_service@127.0.0.1:3300) for replicaset(uuid="b33d17b0-46b7-43a4-9611-98bb29099f4d", master=default_1_1(pico_service@127.0.0.1:3300))
:3300 | 2025-02-17 23:38:29.026 [771631] main/173/lua/vshard.router I> All replicas are ok
:3300 | 2025-02-17 23:38:29.027 [771631] main/173/lua/vshard.router I> Failovering step is finished. Schedule next after 1.000000 seconds
:3300 | 2025-02-17 23:38:29.027 [771631] main/174/lua/vshard.util I> discovery_f has been started
:3300 | 2025-02-17 23:38:29.029 [771631] main/137/main I> subscribed replica 35662ba5-d645-45bb-89b6-0a4553e26627 at fd 47, aka 127.0.0.1:3300, peer of 127.0.0.1:40614
:3300 | 2025-02-17 23:38:29.029 [771631] main/137/main I> remote vclock {1: 150} local vclock {0: 217, 1: 190}
:3300 | 2025-02-17 23:38:29.029 [771631] main/120/to:default_1_1 V> rpc response ignored because caller dropped the future
:3300 | 2025-02-17 23:38:29.029 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> RAFT: message {term: 1, state: follower} from 3
:3300 | 2025-02-17 23:38:29.029 [771631] main/161/applier/pico_service@127.0.0.1:3304 I> leaving orphan mode
:3300 | 2025-02-17 23:38:29.029 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> authenticated
:3300 | 2025-02-17 23:38:29.029 [771631] relay/127.0.0.1:40614/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3300 | 2025-02-17 23:38:29.030 [771631] main/165/vshard.rebalancer/vshard.storage I> Total active bucket count is not equal to total. Possibly a boostrap is not finished yet. Expected 3000, but found 0
:3300 | 2025-02-17 23:38:29.030 [771631] main/165/vshard.rebalancer/vshard.storage I> Some buckets are not active, retry rebalancing later
:3300 | 2025-02-17 23:38:29.030 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> subscribed
:3300 | 2025-02-17 23:38:29.030 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> remote vclock {1: 150} local vclock {0: 217, 1: 190}
:3300 | 2025-02-17 23:38:29.030 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> RAFT: message {term: 1, state: follower} from 2
:3300 | 2025-02-17 23:38:29.030 [771631] main/168/127.0.0.1:3302 (net.box)/vshard.replicaset I> connected to 127.0.0.1:3302
:3300 | 2025-02-17 23:38:29.030 [771631] main/110/main I> subscribed replica caf0b1e6-ae28-4955-9c34-35169d154b1a at fd 40, aka 127.0.0.1:3300, peer of 127.0.0.1:40644
:3300 | 2025-02-17 23:38:29.030 [771631] main/110/main I> remote vclock {1: 150} local vclock {0: 217, 1: 190}
:3300 | 2025-02-17 23:38:29.030 [771631] main/160/applier/pico_service@127.0.0.1:3302 I> leaving orphan mode
:3300 | 2025-02-17 23:38:29.031 [771631] relay/127.0.0.1:40644/101/main I> recover from `/tmp/pytest-of-gmoshkin/pytest-795/tmp0/i1/00000000000000000000.xlog'
:3300 | 2025-02-17 23:38:29.036 [771631] main/174/vshard.discovery.default/vshard.router I> Start aggressive discovery, 3000 buckets are unknown. Discovery works with 1 seconds interval
